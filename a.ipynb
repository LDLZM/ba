{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dgl.utils import expand_as_pair\n",
    "from dgl import function as fn\n",
    "from dgl.base import DGLError\n",
    "from dgl.nn.functional import edge_softmax\n",
    "import numpy as np\n",
    "cat_features = [\"Target\",\n",
    "                \"Type\",\n",
    "                \"Location\"]\n",
    "\n",
    "\n",
    "class PosEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, device, base=10000, bias=0):\n",
    "\n",
    "        super(PosEncoding, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize the posencoding component\n",
    "        :param dim: the encoding dimension \n",
    "\t\t:param device: where to train model\n",
    "\t\t:param base: the encoding base\n",
    "\t\t:param bias: the encoding bias\n",
    "        \"\"\"\n",
    "        p = []\n",
    "        sft = []\n",
    "        for i in range(dim):\n",
    "            b = (i - i % 2) / dim\n",
    "            p.append(base ** -b)\n",
    "            if i % 2:\n",
    "                sft.append(np.pi / 2.0 + bias)\n",
    "            else:\n",
    "                sft.append(bias)\n",
    "        self.device = device\n",
    "        self.sft = torch.tensor(\n",
    "            sft, dtype=torch.float32).view(1, -1).to(device)\n",
    "        self.base = torch.tensor(p, dtype=torch.float32).view(1, -1).to(device)\n",
    "\n",
    "    def forward(self, pos):\n",
    "        with torch.no_grad():\n",
    "            if isinstance(pos, list):\n",
    "                pos = torch.tensor(pos, dtype=torch.float32).to(self.device)\n",
    "            pos = pos.view(-1, 1)\n",
    "            x = pos / self.base + self.sft\n",
    "            return torch.sin(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, df=None, device='cpu', dropout=0.2, in_feats=82, cat_features=None):\n",
    "        \"\"\"\n",
    "        Initialize the attribute embedding and feature learning compoent\n",
    "\n",
    "        :param df: the feature\n",
    "                :param device: where to train model\n",
    "                :param dropout: the dropout rate\n",
    "                :param in_feat: the shape of input feature in dimension 1\n",
    "                :param cat_feature: category features\n",
    "        \"\"\"\n",
    "        super(TransEmbedding, self).__init__()\n",
    "        self.time_pe = PosEncoding(dim=in_feats, device=device, base=100)\n",
    "        #time_emb = time_pe(torch.sin(torch.tensor(df['time_span'].values)/86400*torch.pi))\n",
    "        self.cat_table = nn.ModuleDict({col: nn.Embedding(max(df[col].unique(\n",
    "        ))+1, in_feats).to(device) for col in cat_features if col not in {\"Labels\", \"Time\"}})\n",
    "        self.label_table = nn.Embedding(3, in_feats, padding_idx=2).to(device)\n",
    "        self.time_emb = None\n",
    "        self.emb_dict = None\n",
    "        self.label_emb = None\n",
    "        self.cat_features = cat_features\n",
    "        self.forward_mlp = nn.ModuleList(\n",
    "            [nn.Linear(in_feats, in_feats) for i in range(len(cat_features))])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward_emb(self, df):\n",
    "        if self.emb_dict is None:\n",
    "            self.emb_dict = self.cat_table\n",
    "        # print(self.emb_dict)\n",
    "        # print(df['trans_md'])\n",
    "        support = {col: self.emb_dict[col](\n",
    "            df[col]) for col in self.cat_features if col not in {\"Labels\", \"Time\"}}\n",
    "        #self.time_emb = self.time_pe(torch.sin(torch.tensor(df['time_span'])/86400*torch.pi))\n",
    "        #support['time_span'] = self.time_emb\n",
    "        #support['labels'] = self.label_table(df['labels'])\n",
    "        return support\n",
    "\n",
    "    def forward(self, df):\n",
    "        support = self.forward_emb(df)\n",
    "        output = 0\n",
    "        for i, k in enumerate(support.keys()):\n",
    "            # if k =='time_span':\n",
    "            #    print(df[k].shape)\n",
    "            support[k] = self.dropout(support[k])\n",
    "            support[k] = self.forward_mlp[i](support[k])\n",
    "            output = output + support[k]\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerConv(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 allow_zero_in_degree=False,\n",
    "                 # feat_drop=0.6,\n",
    "                 # attn_drop=0.6,\n",
    "                 skip_feat=True,\n",
    "                 gated=True,\n",
    "                 layer_norm=True,\n",
    "                 activation=nn.PReLU()):\n",
    "        \"\"\"\n",
    "        Initialize the transformer layer.\n",
    "        Attentional weights are jointly optimized in an end-to-end mechanism with graph neural networks and fraud detection networks.\n",
    "            :param in_feat: the shape of input feature\n",
    "            :param out_feats: the shape of output feature\n",
    "            :param num_heads: the number of multi-head attention \n",
    "            :param bias: whether to use bias\n",
    "            :param allow_zero_in_degree: whether to allow zero in degree\n",
    "            :param skip_feat: whether to skip some feature \n",
    "            :param gated: whether to use gate\n",
    "            :param layer_norm: whether to use layer regularization\n",
    "            :param activation: the type of activation function   \n",
    "        \"\"\"\n",
    "\n",
    "        super(TransformerConv, self).__init__()\n",
    "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
    "        self._out_feats = out_feats\n",
    "        self._allow_zero_in_degree = allow_zero_in_degree\n",
    "        self._num_heads = num_heads\n",
    "\n",
    "        self.lin_query = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats*self._num_heads, bias=bias)\n",
    "        self.lin_key = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats*self._num_heads, bias=bias)\n",
    "        self.lin_value = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats*self._num_heads, bias=bias)\n",
    "\n",
    "        #self.feat_dropout = nn.Dropout(p=feat_drop)\n",
    "        #self.attn_dropout = nn.Dropout(p=attn_drop)\n",
    "        if skip_feat:\n",
    "            self.skip_feat = nn.Linear(\n",
    "                self._in_src_feats, self._out_feats*self._num_heads, bias=bias)\n",
    "        else:\n",
    "            self.skip_feat = None\n",
    "        if gated:\n",
    "            self.gate = nn.Linear(\n",
    "                3*self._out_feats*self._num_heads, 1, bias=bias)\n",
    "        else:\n",
    "            self.gate = None\n",
    "        if layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(self._out_feats*self._num_heads)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, graph, feat, get_attention=False):\n",
    "        \"\"\"\n",
    "        Description: Transformer Graph Convolution\n",
    "        :param graph: input graph\n",
    "            :param feat: input feat\n",
    "            :param get_attention: whether to get attention\n",
    "        \"\"\"\n",
    "\n",
    "        graph = graph.local_var()\n",
    "\n",
    "        if not self._allow_zero_in_degree:\n",
    "            if (graph.in_degrees() == 0).any():\n",
    "                raise DGLError('There are 0-in-degree nodes in the graph, '\n",
    "                               'output for those nodes will be invalid. '\n",
    "                               'This is harmful for some applications, '\n",
    "                               'causing silent performance regression. '\n",
    "                               'Adding self-loop on the input graph by '\n",
    "                               'calling `g = dgl.add_self_loop(g)` will resolve '\n",
    "                               'the issue. Setting ``allow_zero_in_degree`` '\n",
    "                               'to be `True` when constructing this module will '\n",
    "                               'suppress the check and let the code run.')\n",
    "\n",
    "        # check if feat is a tuple\n",
    "        if isinstance(feat, tuple):\n",
    "            h_src = feat[0]\n",
    "            h_dst = feat[1]\n",
    "        else:\n",
    "            h_src = feat\n",
    "            h_dst = h_src[:graph.number_of_dst_nodes()]\n",
    "\n",
    "        # Step 0. q, k, v\n",
    "        q_src = self.lin_query(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        k_dst = self.lin_key(h_dst).view(-1, self._num_heads, self._out_feats)\n",
    "        v_src = self.lin_value(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        # Assign features to nodes\n",
    "        graph.srcdata.update({'ft': q_src, 'ft_v': v_src})\n",
    "        graph.dstdata.update({'ft': k_dst})\n",
    "        # Step 1. dot product\n",
    "        graph.apply_edges(fn.u_dot_v('ft', 'ft', 'a'))\n",
    "\n",
    "        # Step 2. edge softmax to compute attention scores\n",
    "        graph.edata['sa'] = edge_softmax(\n",
    "            graph, graph.edata['a'] / self._out_feats**0.5)\n",
    "\n",
    "        # Step 3. Broadcast softmax value to each edge, and aggregate dst node\n",
    "        graph.update_all(fn.u_mul_e('ft_v', 'sa', 'attn'),\n",
    "                         fn.sum('attn', 'agg_u'))\n",
    "\n",
    "        # output results to the destination nodes\n",
    "        rst = graph.dstdata['agg_u'].reshape(-1,\n",
    "                                             self._out_feats*self._num_heads)\n",
    "\n",
    "        if self.skip_feat is not None:\n",
    "            skip_feat = self.skip_feat(feat[:graph.number_of_dst_nodes()])\n",
    "            if self.gate is not None:\n",
    "                gate = torch.sigmoid(\n",
    "                    self.gate(\n",
    "                        torch.concat([skip_feat, rst, skip_feat - rst], dim=-1)))\n",
    "                rst = gate * skip_feat + (1 - gate) * rst\n",
    "            else:\n",
    "                rst = skip_feat + rst\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            rst = self.layer_norm(rst)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            rst = self.activation(rst)\n",
    "\n",
    "        if get_attention:\n",
    "            return rst, graph.edata['sa']\n",
    "        else:\n",
    "            return rst\n",
    "\n",
    "\n",
    "class GraphAttnModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 hidden_dim,\n",
    "                 n_layers,\n",
    "                 n_classes,\n",
    "                 heads,\n",
    "                 activation,\n",
    "                 skip_feat=True,\n",
    "                 gated=True,\n",
    "                 layer_norm=True,\n",
    "                 post_proc=True,\n",
    "                 n2v_feat=True,\n",
    "                 drop=None,\n",
    "                 ref_df=None,\n",
    "                 cat_features=None,\n",
    "                 nei_features=None,\n",
    "                 device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the GTAN-GNN model\n",
    "        :param in_feats: the shape of input feature\n",
    "                :param hidden_dim: model hidden layer dimension\n",
    "                :param n_layers: the number of GTAN layers\n",
    "                :param n_classes: the number of classification\n",
    "                :param heads: the number of multi-head attention \n",
    "                :param activation: the type of activation function\n",
    "                :param skip_feat: whether to skip some feature\n",
    "                :param gated: whether to use gate\n",
    "        :param layer_norm: whether to use layer regularization\n",
    "                :param post_proc: whether to use post processing\n",
    "                :param n2v_feat: whether to use n2v features\n",
    "        :param drop: whether to use drop\n",
    "                :param ref_df: whether to refer other node features\n",
    "                :param cat_features: category features\n",
    "                :param nei_features: neighborhood statistic features\n",
    "        :param device: where to train model\n",
    "        \"\"\"\n",
    "\n",
    "        super(GraphAttnModel, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_classes = n_classes\n",
    "        self.heads = heads\n",
    "        self.activation = activation\n",
    "        #self.input_drop = lambda x: x\n",
    "        self.input_drop = nn.Dropout(drop[0])\n",
    "        self.drop = drop[1]\n",
    "        self.output_drop = nn.Dropout(self.drop)\n",
    "        # self.pn = PairNorm(mode=pairnorm)\n",
    "        if n2v_feat:\n",
    "            self.n2v_mlp = TransEmbedding(\n",
    "                ref_df, device=device, in_feats=in_feats, cat_features=cat_features)\n",
    "        else:\n",
    "            self.n2v_mlp = lambda x: x\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Embedding(\n",
    "            n_classes+1, in_feats, padding_idx=n_classes))\n",
    "        self.layers.append(\n",
    "            nn.Linear(self.in_feats, self.hidden_dim*self.heads[0]))\n",
    "        self.layers.append(\n",
    "            nn.Linear(self.in_feats, self.hidden_dim*self.heads[0]))\n",
    "        self.layers.append(nn.Sequential(nn.BatchNorm1d(self.hidden_dim*self.heads[0]),\n",
    "                                         nn.PReLU(),\n",
    "                                         nn.Dropout(self.drop),\n",
    "                                         nn.Linear(self.hidden_dim *\n",
    "                                                   self.heads[0], in_feats)\n",
    "                                         ))\n",
    "\n",
    "        # build multiple layers\n",
    "        self.layers.append(TransformerConv(in_feats=self.in_feats,\n",
    "                                           out_feats=self.hidden_dim,\n",
    "                                           num_heads=self.heads[0],\n",
    "                                           skip_feat=skip_feat,\n",
    "                                           gated=gated,\n",
    "                                           layer_norm=layer_norm,\n",
    "                                           activation=self.activation))\n",
    "\n",
    "        for l in range(0, (self.n_layers - 1)):\n",
    "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
    "            self.layers.append(TransformerConv(in_feats=self.hidden_dim * self.heads[l - 1],\n",
    "                                               out_feats=self.hidden_dim,\n",
    "                                               num_heads=self.heads[l],\n",
    "                                               skip_feat=skip_feat,\n",
    "                                               gated=gated,\n",
    "                                               layer_norm=layer_norm,\n",
    "                                               activation=self.activation))\n",
    "        if post_proc:\n",
    "            self.layers.append(nn.Sequential(nn.Linear(self.hidden_dim * self.heads[-1], self.hidden_dim * self.heads[-1]),\n",
    "                                             nn.BatchNorm1d(\n",
    "                                                 self.hidden_dim * self.heads[-1]),\n",
    "                                             nn.PReLU(),\n",
    "                                             nn.Dropout(self.drop),\n",
    "                                             nn.Linear(self.hidden_dim * self.heads[-1], self.n_classes)))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(self.hidden_dim *\n",
    "                               self.heads[-1], self.n_classes))\n",
    "\n",
    "    def forward(self, blocks, features, labels, n2v_feat=None):\n",
    "        \"\"\"\n",
    "        :param blocks: train blocks\n",
    "        :param features: train features  (|input|, feta_dim)\n",
    "        :param labels: train labels (|input|, )\n",
    "        :param n2v_feat: whether to use n2v features \n",
    "        \"\"\"\n",
    "\n",
    "        if n2v_feat is None:\n",
    "            h = features\n",
    "        else:\n",
    "            h = self.n2v_mlp(n2v_feat)\n",
    "            h = features + h\n",
    "\n",
    "        label_embed = self.input_drop(self.layers[0](labels))\n",
    "        label_embed = self.layers[1](h) + self.layers[2](label_embed)\n",
    "        label_embed = self.layers[3](label_embed)\n",
    "        h = h + label_embed  # residual\n",
    "\n",
    "        for l in range(self.n_layers):\n",
    "            h = self.output_drop(self.layers[l+4](blocks[l], h))\n",
    "\n",
    "        logits = self.layers[-1](h)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入向量的形状: torch.Size([4, 8])\n",
      "填充索引对应的嵌入向量: tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "tensor([[ 2.2289,  1.2767,  1.0011, -1.3499,  0.3960,  1.4382,  0.3253, -0.5629],\n",
      "        [-0.9021,  0.8298,  1.6002,  0.5777, -0.9464,  0.5399,  0.6111,  1.4507],\n",
      "        [-0.2650,  0.4222,  0.3282, -0.0244, -0.4252, -0.4598, -0.4068,  0.3397],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设分类任务有 3 个类别\n",
    "n_classes = 3\n",
    "in_feats = 8\n",
    "padding_idx = n_classes\n",
    "\n",
    "# 创建嵌入层\n",
    "embedding = nn.Embedding(n_classes + 1, in_feats, padding_idx=padding_idx)\n",
    "\n",
    "# 示例输入标签，包含填充索引\n",
    "labels = torch.tensor([0, 1, 2, 3])  # 这里的 3 是填充索引\n",
    "\n",
    "# 进行嵌入操作\n",
    "label_embed = embedding(labels)\n",
    "\n",
    "print(\"嵌入向量的形状:\", label_embed.shape)\n",
    "print(\"填充索引对应的嵌入向量:\", label_embed[-1])\n",
    "\n",
    "print(label_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class early_stopper(object):\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Initialize the early stopper\n",
    "        :param patience: the maximum number of rounds tolerated\n",
    "        :param verbose: whether to stop early\n",
    "        :param delta: the regularization factor\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_value = None\n",
    "        self.best_cv = None\n",
    "        self.is_earlystop = False\n",
    "        self.count = 0\n",
    "        self.best_model = None\n",
    "        # self.val_preds = []\n",
    "        # self.val_logits = []\n",
    "\n",
    "    def earlystop(self, loss, model=None):  # , preds, logits):\n",
    "        \"\"\"\n",
    "        :param loss: the loss score on validation set\n",
    "        :param model: the model\n",
    "        \"\"\"\n",
    "        value = -loss\n",
    "        cv = loss\n",
    "        # value = ap\n",
    "\n",
    "        if self.best_value is None:\n",
    "            self.best_value = value\n",
    "            self.best_cv = cv\n",
    "            self.best_model = copy.deepcopy(model).to('cpu')\n",
    "            # self.val_preds = preds\n",
    "            # self.val_logits = logits\n",
    "        elif value < self.best_value + self.delta:\n",
    "            self.count += 1\n",
    "            if self.verbose:\n",
    "                print('EarlyStoper count: {:02d}'.format(self.count))\n",
    "            if self.count >= self.patience:\n",
    "                self.is_earlystop = True\n",
    "        else:\n",
    "            self.best_value = value\n",
    "            self.best_cv = cv\n",
    "            self.best_model = copy.deepcopy(model).to('cpu')\n",
    "            # self.val_preds = preds\n",
    "            # self.val_logits = logits\n",
    "            self.count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def load_lpa_subtensor(node_feat, work_node_feat, labels, seeds, input_nodes, device):\n",
    "    batch_inputs = node_feat[input_nodes].to(device)\n",
    "    batch_work_inputs = {i: work_node_feat[i][input_nodes].to(\n",
    "        device) for i in work_node_feat if i not in {\"Labels\"}}\n",
    "    # for i in batch_work_inputs:\n",
    "    #    print(batch_work_inputs[i].shape)\n",
    "    batch_labels = labels[seeds].to(device)\n",
    "    train_labels = copy.deepcopy(labels)\n",
    "    propagate_labels = train_labels[input_nodes]\n",
    "    propagate_labels[:seeds.shape[0]] = 2\n",
    "    return batch_inputs, batch_work_inputs, batch_labels, propagate_labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "import os\n",
    "from sklearn.metrics import average_precision_score, f1_score, roc_auc_score\n",
    "import torch.optim as optim\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, QuantileTransformer\n",
    "from dgl.dataloading import MultiLayerFullNeighborSampler\n",
    "from dgl.dataloading import NodeDataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "def gtan_main(feat_df, graph, train_idx, test_idx, labels, args, cat_features):\n",
    "    print('-------------------------------------')\n",
    "    device = args['device'] # 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    graph = graph.to(device) # move graph to device\n",
    "    oof_predictions = torch.from_numpy(\n",
    "        np.zeros([len(feat_df), 2])).float().to(device) \n",
    "    #oof_predictions 用于存储每个折叠（fold）中验证集样本的预测结果。\n",
    "    # 每次训练一个模型并在验证集上进行预测时，预测结果会填充到 oof_predictions 中对应的位置。\n",
    "    # 这样，在所有折叠的训练和验证完成后，oof_predictions 就包含了整个训练集在交叉验证过程中的所有袋外预测结果。\n",
    "    # 这些预测结果可以用于评估模型在训练集上的性能\n",
    "    test_predictions = torch.from_numpy(\n",
    "        np.zeros([len(feat_df), 2])).float().to(device)\n",
    "\n",
    "\n",
    "    kfold = StratifiedKFold(\n",
    "        n_splits=args['n_fold'], shuffle=True, random_state=args['seed'])\n",
    "\n",
    "\n",
    "    y_target = labels.iloc[train_idx].values\n",
    "    print()\n",
    "    print(feat_df.head())\n",
    "    print()\n",
    "    print(feat_df.values)\n",
    "    print()\n",
    "    num_feat = torch.from_numpy(feat_df.values).float().to(device) \n",
    "    cat_feat = {col: torch.from_numpy(feat_df[col].values).long().to(\n",
    "        device) for col in cat_features}\n",
    "    \n",
    "    \n",
    "    y = labels\n",
    "\n",
    "    labels = torch.from_numpy(y.values).long().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kfold.split(feat_df.iloc[train_idx], y_target)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        trn_ind, val_ind = torch.from_numpy(np.array(train_idx)[trn_idx]).long().to(\n",
    "            device), torch.from_numpy(np.array(train_idx)[val_idx]).long().to(device)\n",
    "\n",
    "        train_sampler = MultiLayerFullNeighborSampler(args['n_layers'])\n",
    "        train_dataloader = NodeDataLoader(graph,# 图数据\n",
    "                                          trn_ind, # 训练集节点索引\n",
    "                                          train_sampler, # 采样器\n",
    "                                          device=device, # 设备\n",
    "                                          use_ddp=False, # 是否使用分布式训练\n",
    "                                          batch_size=args['batch_size'], # 批大小\n",
    "                                          shuffle=True, # 是否打乱数据\n",
    "                                          drop_last=False, # 是否丢弃最后一批\n",
    "                                          num_workers=0 # 工作线程数\n",
    "                                          )\n",
    "        val_sampler = MultiLayerFullNeighborSampler(args['n_layers'])\n",
    "        val_dataloader = NodeDataLoader(graph, # 图数据\n",
    "                                        val_ind, # 验证集节点索引\n",
    "                                        val_sampler, # 采样器\n",
    "                                        use_ddp=False, # 是否使用分布式训练\n",
    "                                        device=device, # 设备\n",
    "                                        batch_size=args['batch_size'], # 批大小\n",
    "                                        shuffle=True, # 是否打乱数据\n",
    "                                        drop_last=False, # 是否丢弃最后一批\n",
    "                                        num_workers=0 # 工作线程数\n",
    "                                        )\n",
    "        # TODO\n",
    "        model = GraphAttnModel(in_feats=feat_df.shape[1],\n",
    "                               # 为什么要整除4？\n",
    "                               hidden_dim=args['hid_dim']//4,\n",
    "                               n_classes=2,\n",
    "                               heads=[4]*args['n_layers'],  # [4,4,4]\n",
    "                               activation=nn.PReLU(),\n",
    "                               n_layers=args['n_layers'],\n",
    "                               drop=args['dropout'],\n",
    "                               device=device,\n",
    "                               gated=args['gated'],\n",
    "                               ref_df=feat_df,\n",
    "                               cat_features=cat_feat).to(device)\n",
    "        \n",
    "        lr = args['lr'] * np.sqrt(args['batch_size']/1024)  # 0.00075\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                               weight_decay=args['wd'])\n",
    "        lr_scheduler = MultiStepLR(optimizer=optimizer, milestones=[\n",
    "                                   4000, 12000], gamma=0.3)\n",
    "\n",
    "        earlystoper = early_stopper(\n",
    "            patience=args['early_stopping'], verbose=True)\n",
    "        start_epoch, max_epochs = 0, 2000\n",
    "        for epoch in range(start_epoch, args['max_epochs']):\n",
    "            train_loss_list = []\n",
    "            # train_acc_list = []\n",
    "            model.train()\n",
    "            for step, (input_nodes, seeds, blocks) in enumerate(train_dataloader):\n",
    "                batch_inputs, batch_work_inputs, batch_labels, lpa_labels = load_lpa_subtensor(num_feat, cat_feat, labels,\n",
    "                                                                                               seeds, input_nodes, device)\n",
    "                # (|input|, feat_dim); null; (|batch|,); (|input|,)\n",
    "                blocks = [block.to(device) for block in blocks]\n",
    "                train_batch_logits = model(\n",
    "                    blocks, batch_inputs, lpa_labels, batch_work_inputs)\n",
    "                mask = batch_labels == 2\n",
    "                train_batch_logits = train_batch_logits[~mask]\n",
    "                batch_labels = batch_labels[~mask]\n",
    "                # batch_labels[mask] = 0\n",
    "\n",
    "                train_loss = loss_fn(train_batch_logits, batch_labels)\n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                train_loss_list.append(train_loss.cpu().detach().numpy())\n",
    "\n",
    "                if step % 10 == 0:\n",
    "                    tr_batch_pred = torch.sum(torch.argmax(train_batch_logits.clone(\n",
    "                    ).detach(), dim=1) == batch_labels) / batch_labels.shape[0]\n",
    "                    score = torch.softmax(train_batch_logits.clone().detach(), dim=1)[\n",
    "                        :, 1].cpu().numpy()\n",
    "\n",
    "                    # if (len(np.unique(score)) == 1):\n",
    "                    #     print(\"all same prediction!\")\n",
    "                    try:\n",
    "                        print('In epoch:{:03d}|batch:{:04d}, train_loss:{:4f}, '\n",
    "                              'train_ap:{:.4f}, train_acc:{:.4f}, train_auc:{:.4f}'.format(epoch, step,\n",
    "                                                                                           np.mean(\n",
    "                                                                                               train_loss_list),\n",
    "                                                                                           average_precision_score(\n",
    "                                                                                               batch_labels.cpu().numpy(), score),\n",
    "                                                                                           tr_batch_pred.detach(),\n",
    "                                                                                           roc_auc_score(batch_labels.cpu().numpy(), score)))\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            # mini-batch for validation\n",
    "            val_loss_list = 0\n",
    "            val_acc_list = 0\n",
    "            val_all_list = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for step, (input_nodes, seeds, blocks) in enumerate(val_dataloader):\n",
    "                    batch_inputs, batch_work_inputs, batch_labels, lpa_labels = load_lpa_subtensor(num_feat, cat_feat, labels,\n",
    "                                                                                                   seeds, input_nodes, device)\n",
    "\n",
    "                    blocks = [block.to(device) for block in blocks]\n",
    "                    val_batch_logits = model(\n",
    "                        blocks, batch_inputs, lpa_labels, batch_work_inputs)\n",
    "                    oof_predictions[seeds] = val_batch_logits\n",
    "                    mask = batch_labels == 2\n",
    "                    val_batch_logits = val_batch_logits[~mask]\n",
    "                    batch_labels = batch_labels[~mask]\n",
    "                    # batch_labels[mask] = 0\n",
    "                    val_loss_list = val_loss_list + \\\n",
    "                        loss_fn(val_batch_logits, batch_labels)\n",
    "                    # val_all_list += 1\n",
    "                    val_batch_pred = torch.sum(torch.argmax(\n",
    "                        val_batch_logits, dim=1) == batch_labels) / torch.tensor(batch_labels.shape[0])\n",
    "                    val_acc_list = val_acc_list + val_batch_pred * \\\n",
    "                        torch.tensor(batch_labels.shape[0])\n",
    "                    val_all_list = val_all_list + batch_labels.shape[0]\n",
    "                    if step % 10 == 0:\n",
    "                        score = torch.softmax(val_batch_logits.clone().detach(), dim=1)[\n",
    "                            :, 1].cpu().numpy()\n",
    "                        try:\n",
    "                            print('In epoch:{:03d}|batch:{:04d}, val_loss:{:4f}, val_ap:{:.4f}, '\n",
    "                                  'val_acc:{:.4f}, val_auc:{:.4f}'.format(epoch,\n",
    "                                                                          step,\n",
    "                                                                          val_loss_list/val_all_list,\n",
    "                                                                          average_precision_score(\n",
    "                                                                              batch_labels.cpu().numpy(), score),\n",
    "                                                                          val_batch_pred.detach(),\n",
    "                                                                          roc_auc_score(batch_labels.cpu().numpy(), score)))\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "            # val_acc_list/val_all_list, model)\n",
    "            earlystoper.earlystop(val_loss_list/val_all_list, model)\n",
    "            if earlystoper.is_earlystop:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "        print(\"Best val_loss is: {:.7f}\".format(earlystoper.best_cv))\n",
    "        test_ind = torch.from_numpy(np.array(test_idx)).long().to(device)\n",
    "        test_sampler = MultiLayerFullNeighborSampler(args['n_layers'])\n",
    "        test_dataloader = NodeDataLoader(graph,\n",
    "                                         test_ind,\n",
    "                                         test_sampler,\n",
    "                                         use_ddp=False,\n",
    "                                         device=device,\n",
    "                                         batch_size=args['batch_size'],\n",
    "                                         shuffle=True,\n",
    "                                         drop_last=False,\n",
    "                                         num_workers=0,\n",
    "                                         )\n",
    "        b_model = earlystoper.best_model.to(device)\n",
    "        b_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, (input_nodes, seeds, blocks) in enumerate(test_dataloader):\n",
    "                # print(input_nodes)\n",
    "                batch_inputs, batch_work_inputs, batch_labels, lpa_labels = load_lpa_subtensor(num_feat, cat_feat, labels,\n",
    "                                                                                               seeds, input_nodes, device)\n",
    "\n",
    "                blocks = [block.to(device) for block in blocks]\n",
    "                test_batch_logits = b_model(\n",
    "                    blocks, batch_inputs, lpa_labels, batch_work_inputs)\n",
    "                test_predictions[seeds] = test_batch_logits\n",
    "                test_batch_pred = torch.sum(torch.argmax(\n",
    "                    test_batch_logits, dim=1) == batch_labels) / torch.tensor(batch_labels.shape[0])\n",
    "                if step % 10 == 0:\n",
    "                    print('In test batch:{:04d}'.format(step))\n",
    "    mask = y_target == 2\n",
    "    y_target[mask] = 0\n",
    "    my_ap = average_precision_score(y_target, torch.softmax(\n",
    "        oof_predictions, dim=1).cpu()[train_idx, 1])\n",
    "    print(\"NN out of fold AP is:\", my_ap)\n",
    "    b_models, val_gnn_0, test_gnn_0 = earlystoper.best_model.to(\n",
    "        'cpu'), oof_predictions, test_predictions\n",
    "\n",
    "    test_score = torch.softmax(test_gnn_0, dim=1)[test_idx, 1].cpu().numpy()\n",
    "    y_target = labels[test_idx].cpu().numpy()\n",
    "    test_score1 = torch.argmax(test_gnn_0, dim=1)[test_idx].cpu().numpy()\n",
    "\n",
    "    mask = y_target != 2\n",
    "    test_score = test_score[mask]\n",
    "    y_target = y_target[mask]\n",
    "    test_score1 = test_score1[mask]\n",
    "\n",
    "    print(\"test AUC:\", roc_auc_score(y_target, test_score))\n",
    "    print(\"test f1:\", f1_score(y_target, test_score1, average=\"macro\"))\n",
    "    print(\"test AP:\", average_precision_score(y_target, test_score))\n",
    "\n",
    "\n",
    "def load_gtan_data(dataset: str, test_size: float):\n",
    "    \"\"\"\n",
    "    Load graph, feature, and label given dataset name\n",
    "    :param dataset: the dataset name\n",
    "    :param test_size: the size of test set\n",
    "    :returns: feature, label, graph, category features\n",
    "    \"\"\"\n",
    "    # prefix = './antifraud/data/'\n",
    "    prefix = os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"data/\")\n",
    "    if dataset == \"S-FFSD\":\n",
    "        cat_features = [\"Target\", \"Location\", \"Type\"]\n",
    "\n",
    "        df = pd.read_csv(prefix + \"S-FFSDneofull.csv\")\n",
    "        df = df.loc[:, ~df.columns.str.contains('Unnamed')]\n",
    "        data = df[df[\"Labels\"] <= 2]\n",
    "        data = data.reset_index(drop=True)\n",
    "        out = []\n",
    "        alls = []\n",
    "        allt = []\n",
    "        pair = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "        for column in pair:\n",
    "            src, tgt = [], []\n",
    "            edge_per_trans = 3\n",
    "            for c_id, c_df in data.groupby(column):\n",
    "                c_df = c_df.sort_values(by=\"Time\")\n",
    "                df_len = len(c_df)\n",
    "                sorted_idxs = c_df.index\n",
    "                src.extend([sorted_idxs[i] for i in range(df_len)\n",
    "                            for j in range(edge_per_trans) if i + j < df_len])\n",
    "                tgt.extend([sorted_idxs[i+j] for i in range(df_len)\n",
    "                            for j in range(edge_per_trans) if i + j < df_len])\n",
    "            alls.extend(src)\n",
    "            allt.extend(tgt)\n",
    "        alls = np.array(alls)\n",
    "        allt = np.array(allt)\n",
    "        g = dgl.graph((alls, allt))\n",
    "\n",
    "      \n",
    "        cal_list = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "        for col in cal_list:\n",
    "            le = LabelEncoder()\n",
    "            data[col] = le.fit_transform(data[col].apply(str).values)\n",
    "        feat_data = data.drop(\"Labels\", axis=1)\n",
    "      \n",
    "        labels = data[\"Labels\"]\n",
    "        ###\n",
    "        feat_data.to_csv(prefix + \"S-FFSD_feat_data.csv\", index=None)\n",
    "        labels.to_csv(prefix + \"S-FFSD_label_data.csv\", index=None)\n",
    "        ###\n",
    "        index = list(range(len(labels)))\n",
    "        g.ndata['label'] = torch.from_numpy(\n",
    "            labels.to_numpy()).to(torch.long)\n",
    "        g.ndata['feat'] = torch.from_numpy(\n",
    "            feat_data.to_numpy()).to(torch.float32)\n",
    "        graph_path = prefix+\"graph-{}.bin\".format(dataset)\n",
    "        dgl.data.utils.save_graphs(graph_path, [g])\n",
    "\n",
    "        train_idx, test_idx, y_train, y_test = train_test_split(index, labels, stratify=labels, test_size=test_size/2,\n",
    "                                                                random_state=2, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return feat_data, labels, train_idx, test_idx, g, cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "from config import Config\n",
    "from feature_engineering.data_engineering import data_engineer_benchmark, span_data_2d, span_data_3d\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import pickle\n",
    "import dgl\n",
    "from scipy.io import loadmat\n",
    "import yaml\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "# sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter,\n",
    "                            conflict_handler='resolve')\n",
    "    parser.add_argument(\"--method\", default='gtan')  # specify which method to use\n",
    "    method = vars(parser.parse_args())['method']  # dict\n",
    "\n",
    "    # if method in ['']:\n",
    "    #     yaml_file = \"config/base_cfg.yaml\"\n",
    "    if method in ['mcnn']:\n",
    "        yaml_file = \"config/mcnn_cfg.yaml\"\n",
    "    elif method in ['stan']:\n",
    "        yaml_file = \"config/stan_cfg.yaml\"\n",
    "    elif method in ['stan_2d']:\n",
    "        yaml_file = \"config/stan_2d_cfg.yaml\"\n",
    "    elif method in ['stagn']:\n",
    "        yaml_file = \"config/stagn_cfg.yaml\"\n",
    "    elif method in ['gtan']:\n",
    "        print(\"gtan was chosen\")\n",
    "        yaml_file = \"gtan_cfg.yaml\"\n",
    "    elif method in ['rgtan']:\n",
    "        yaml_file = \"config/rgtan_cfg.yaml\"\n",
    "    elif method in ['hogrl']:\n",
    "        yaml_file = \"config/hogrl_cfg.yaml\"\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(\"Unsupported method.\")\n",
    "\n",
    "    # config = Config().get_config()\n",
    "    with open(yaml_file) as file:\n",
    "        args = yaml.safe_load(file)\n",
    "    args['method'] = method\n",
    "    return args\n",
    "\n",
    "\n",
    "def base_load_data(args: dict):\n",
    "    # load S-FFSD dataset for base models\n",
    "    data_path = \"data/S-FFSD.csv\"\n",
    "    feat_df = pd.read_csv(data_path)\n",
    "    train_size = 1 - args['test_size']\n",
    "    method = args['method']\n",
    "    # for ICONIP16 & AAAI20\n",
    "    if args['method'] == 'stan':\n",
    "        if os.path.exists(\"data/tel_3d.npy\"):\n",
    "            return\n",
    "        features, labels = span_data_3d(feat_df)\n",
    "    else:\n",
    "        if os.path.exists(\"data/tel_2d.npy\"):\n",
    "            return\n",
    "        features, labels = span_data_2d(feat_df)\n",
    "    num_trans = len(feat_df)\n",
    "    trf, tef, trl, tel = train_test_split(\n",
    "        features, labels, train_size=train_size, stratify=labels, shuffle=True)\n",
    "    trf_file, tef_file, trl_file, tel_file = args['trainfeature'], args[\n",
    "        'testfeature'], args['trainlabel'], args['testlabel']\n",
    "    np.save(trf_file, trf)\n",
    "    np.save(tef_file, tef)\n",
    "    np.save(trl_file, trl)\n",
    "    np.save(tel_file, tel)\n",
    "    return\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    if args['method'] == 'mcnn':\n",
    "        pass\n",
    "    elif args['method'] == 'stan_2d':\n",
    "        pass\n",
    "    elif args['method'] == 'stan':\n",
    "       pass\n",
    "    elif args['method'] == 'stagn':\n",
    "       pass\n",
    "    elif args['method'] == 'gtan':\n",
    "        from methods.gtan.gtan_main import gtan_main, load_gtan_data\n",
    "        feat_data, labels, train_idx, test_idx, g, cat_features = load_gtan_data(\n",
    "            args['dataset'], args['test_size'])\n",
    "        # feat_data用于存储数据集的特征数据，通常是一个矩阵或数组，每一行代表一个样本的特征向量。\n",
    "        # labels 标签\n",
    "        # train_idx, \n",
    "        # test_idx,\n",
    "        # g, \n",
    "        # cat_features 类别特征\n",
    "        gtan_main(\n",
    "            feat_data, g, train_idx, test_idx, labels, args, cat_features)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(parse_args())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dgl.utils import expand_as_pair\n",
    "from dgl import function as fn\n",
    "from dgl.base import DGLError\n",
    "from dgl.nn.functional import edge_softmax\n",
    "import numpy as np\n",
    "cat_features = [\"Target\",\n",
    "                \"Type\",\n",
    "                \"Location\"]\n",
    "\n",
    "\n",
    "class PosEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, device, base=10000, bias=0):\n",
    "\n",
    "        super(PosEncoding, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize the posencoding component\n",
    "        :param dim: the encoding dimension \n",
    "\t\t:param device: where to train model\n",
    "\t\t:param base: the encoding base\n",
    "\t\t:param bias: the encoding bias\n",
    "        \"\"\"\n",
    "        p = []\n",
    "        sft = []\n",
    "        for i in range(dim):\n",
    "            b = (i - i % 2) / dim\n",
    "            p.append(base ** -b)\n",
    "            if i % 2:\n",
    "                sft.append(np.pi / 2.0 + bias)\n",
    "            else:\n",
    "                sft.append(bias)\n",
    "        self.device = device\n",
    "        self.sft = torch.tensor(\n",
    "            sft, dtype=torch.float32).view(1, -1).to(device)\n",
    "        self.base = torch.tensor(p, dtype=torch.float32).view(1, -1).to(device)\n",
    "\n",
    "    def forward(self, pos):\n",
    "        with torch.no_grad():\n",
    "            if isinstance(pos, list):\n",
    "                pos = torch.tensor(pos, dtype=torch.float32).to(self.device)\n",
    "            pos = pos.view(-1, 1)\n",
    "            x = pos / self.base + self.sft\n",
    "            return torch.sin(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, df=None, device='cpu', dropout=0.2, in_feats=82, cat_features=None):\n",
    "        \"\"\"\n",
    "        Initialize the attribute embedding and feature learning compoent\n",
    "\n",
    "        :param df: the feature\n",
    "                :param device: where to train model\n",
    "                :param dropout: the dropout rate\n",
    "                :param in_feat: the shape of input feature in dimension 1\n",
    "                :param cat_feature: category features\n",
    "        \"\"\"\n",
    "        super(TransEmbedding, self).__init__()\n",
    "        self.time_pe = PosEncoding(dim=in_feats, device=device, base=100)\n",
    "        #time_emb = time_pe(torch.sin(torch.tensor(df['time_span'].values)/86400*torch.pi))\n",
    "        self.cat_table = nn.ModuleDict({col: nn.Embedding(max(df[col].unique(\n",
    "        ))+1, in_feats).to(device) for col in cat_features if col not in {\"Labels\", \"Time\"}})\n",
    "        self.label_table = nn.Embedding(3, in_feats, padding_idx=2).to(device)\n",
    "        self.time_emb = None\n",
    "        self.emb_dict = None\n",
    "        self.label_emb = None\n",
    "        self.cat_features = cat_features\n",
    "        self.forward_mlp = nn.ModuleList(\n",
    "            [nn.Linear(in_feats, in_feats) for i in range(len(cat_features))])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward_emb(self, df):\n",
    "        if self.emb_dict is None:\n",
    "            self.emb_dict = self.cat_table\n",
    "        # print(self.emb_dict)\n",
    "        # print(df['trans_md'])\n",
    "        support = {col: self.emb_dict[col](\n",
    "            df[col]) for col in self.cat_features if col not in {\"Labels\", \"Time\"}}\n",
    "        #self.time_emb = self.time_pe(torch.sin(torch.tensor(df['time_span'])/86400*torch.pi))\n",
    "        #support['time_span'] = self.time_emb\n",
    "        #support['labels'] = self.label_table(df['labels'])\n",
    "        return support\n",
    "\n",
    "    def forward(self, df):\n",
    "        support = self.forward_emb(df)\n",
    "        output = 0\n",
    "        for i, k in enumerate(support.keys()):\n",
    "            # if k =='time_span':\n",
    "            #    print(df[k].shape)\n",
    "            support[k] = self.dropout(support[k])\n",
    "            support[k] = self.forward_mlp[i](support[k])\n",
    "            output = output + support[k]\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerConv(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 allow_zero_in_degree=False,\n",
    "                 # feat_drop=0.6,\n",
    "                 # attn_drop=0.6,\n",
    "                 skip_feat=True,\n",
    "                 gated=True,\n",
    "                 layer_norm=True,\n",
    "                 activation=nn.PReLU()):\n",
    "        \"\"\"\n",
    "        Initialize the transformer layer.\n",
    "        Attentional weights are jointly optimized in an end-to-end mechanism with graph neural networks and fraud detection networks.\n",
    "            :param in_feat: the shape of input feature\n",
    "            :param out_feats: the shape of output feature\n",
    "            :param num_heads: the number of multi-head attention \n",
    "            :param bias: whether to use bias\n",
    "            :param allow_zero_in_degree: whether to allow zero in degree\n",
    "            :param skip_feat: whether to skip some feature \n",
    "            :param gated: whether to use gate\n",
    "            :param layer_norm: whether to use layer regularization\n",
    "            :param activation: the type of activation function   \n",
    "        \"\"\"\n",
    "\n",
    "        super(TransformerConv, self).__init__()\n",
    "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
    "        self._out_feats = out_feats\n",
    "        self._allow_zero_in_degree = allow_zero_in_degree\n",
    "        self._num_heads = num_heads\n",
    "\n",
    "        self.lin_query = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats*self._num_heads, bias=bias)\n",
    "        self.lin_key = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats*self._num_heads, bias=bias)\n",
    "        self.lin_value = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats*self._num_heads, bias=bias)\n",
    "\n",
    "        #self.feat_dropout = nn.Dropout(p=feat_drop)\n",
    "        #self.attn_dropout = nn.Dropout(p=attn_drop)\n",
    "        if skip_feat:\n",
    "            self.skip_feat = nn.Linear(\n",
    "                self._in_src_feats, self._out_feats*self._num_heads, bias=bias)\n",
    "        else:\n",
    "            self.skip_feat = None\n",
    "        if gated:\n",
    "            self.gate = nn.Linear(\n",
    "                3*self._out_feats*self._num_heads, 1, bias=bias)\n",
    "        else:\n",
    "            self.gate = None\n",
    "        if layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(self._out_feats*self._num_heads)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, graph, feat, get_attention=False):\n",
    "        \"\"\"\n",
    "        Description: Transformer Graph Convolution\n",
    "        :param graph: input graph\n",
    "            :param feat: input feat\n",
    "            :param get_attention: whether to get attention\n",
    "        \"\"\"\n",
    "\n",
    "        graph = graph.local_var()\n",
    "\n",
    "        if not self._allow_zero_in_degree:\n",
    "            if (graph.in_degrees() == 0).any():\n",
    "                raise DGLError('There are 0-in-degree nodes in the graph, '\n",
    "                               'output for those nodes will be invalid. '\n",
    "                               'This is harmful for some applications, '\n",
    "                               'causing silent performance regression. '\n",
    "                               'Adding self-loop on the input graph by '\n",
    "                               'calling `g = dgl.add_self_loop(g)` will resolve '\n",
    "                               'the issue. Setting ``allow_zero_in_degree`` '\n",
    "                               'to be `True` when constructing this module will '\n",
    "                               'suppress the check and let the code run.')\n",
    "\n",
    "        # check if feat is a tuple\n",
    "        if isinstance(feat, tuple):\n",
    "            h_src = feat[0]\n",
    "            h_dst = feat[1]\n",
    "        else:\n",
    "            h_src = feat\n",
    "            h_dst = h_src[:graph.number_of_dst_nodes()]\n",
    "\n",
    "        # Step 0. q, k, v\n",
    "        q_src = self.lin_query(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        k_dst = self.lin_key(h_dst).view(-1, self._num_heads, self._out_feats)\n",
    "        v_src = self.lin_value(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        # Assign features to nodes\n",
    "        graph.srcdata.update({'ft': q_src, 'ft_v': v_src})\n",
    "        graph.dstdata.update({'ft': k_dst})\n",
    "        # Step 1. dot product\n",
    "        graph.apply_edges(fn.u_dot_v('ft', 'ft', 'a'))\n",
    "\n",
    "        # Step 2. edge softmax to compute attention scores\n",
    "        graph.edata['sa'] = edge_softmax(\n",
    "            graph, graph.edata['a'] / self._out_feats**0.5)\n",
    "\n",
    "        # Step 3. Broadcast softmax value to each edge, and aggregate dst node\n",
    "        graph.update_all(fn.u_mul_e('ft_v', 'sa', 'attn'),\n",
    "                         fn.sum('attn', 'agg_u'))\n",
    "\n",
    "        # output results to the destination nodes\n",
    "        rst = graph.dstdata['agg_u'].reshape(-1,\n",
    "                                             self._out_feats*self._num_heads)\n",
    "\n",
    "        if self.skip_feat is not None:\n",
    "            skip_feat = self.skip_feat(feat[:graph.number_of_dst_nodes()])\n",
    "            if self.gate is not None:\n",
    "                gate = torch.sigmoid(\n",
    "                    self.gate(\n",
    "                        torch.concat([skip_feat, rst, skip_feat - rst], dim=-1)))\n",
    "                rst = gate * skip_feat + (1 - gate) * rst\n",
    "            else:\n",
    "                rst = skip_feat + rst\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            rst = self.layer_norm(rst)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            rst = self.activation(rst)\n",
    "\n",
    "        if get_attention:\n",
    "            return rst, graph.edata['sa']\n",
    "        else:\n",
    "            return rst\n",
    "\n",
    "\n",
    "class GraphAttnModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 hidden_dim,\n",
    "                 n_layers,\n",
    "                 n_classes,\n",
    "                 heads,\n",
    "                 activation,\n",
    "                 skip_feat=True,\n",
    "                 gated=True,\n",
    "                 layer_norm=True,\n",
    "                 post_proc=True,\n",
    "                 n2v_feat=True,\n",
    "                 drop=None,\n",
    "                 ref_df=None,\n",
    "                 cat_features=None,\n",
    "                 nei_features=None,\n",
    "                 device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the GTAN-GNN model \n",
    "        :param in_feats: the shape of input feature\n",
    "                :param hidden_dim: model hidden layer dimension\n",
    "                :param n_layers: the number of GTAN layers\n",
    "                :param n_classes: the number of classification\n",
    "                :param heads: the number of multi-head attention \n",
    "                :param activation: the type of activation function\n",
    "                :param skip_feat: whether to skip some feature\n",
    "                :param gated: whether to use gate\n",
    "        :param layer_norm: whether to use layer regularization\n",
    "                :param post_proc: whether to use post processing\n",
    "                :param n2v_feat: whether to use n2v features\n",
    "        :param drop: whether to use drop\n",
    "                :param ref_df: whether to refer other node features\n",
    "                :param cat_features: category features\n",
    "                :param nei_features: neighborhood statistic features\n",
    "        :param device: where to train model\n",
    "        \"\"\"\n",
    "\n",
    "        super(GraphAttnModel, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_classes = n_classes\n",
    "        self.heads = heads\n",
    "        self.activation = activation\n",
    "        #self.input_drop = lambda x: x\n",
    "        self.input_drop = nn.Dropout(drop[0])\n",
    "        self.drop = drop[1]\n",
    "        self.output_drop = nn.Dropout(self.drop)\n",
    "        # self.pn = PairNorm(mode=pairnorm)\n",
    "        if n2v_feat:\n",
    "            self.n2v_mlp = TransEmbedding(\n",
    "                ref_df, device=device, in_feats=in_feats, cat_features=cat_features)\n",
    "        else:\n",
    "            self.n2v_mlp = lambda x: x\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Embedding(\n",
    "            n_classes+1, in_feats, padding_idx=n_classes))\n",
    "        self.layers.append(\n",
    "            nn.Linear(self.in_feats, self.hidden_dim*self.heads[0]))\n",
    "        self.layers.append(\n",
    "            nn.Linear(self.in_feats, self.hidden_dim*self.heads[0]))\n",
    "        self.layers.append(nn.Sequential(nn.BatchNorm1d(self.hidden_dim*self.heads[0]),\n",
    "                                         nn.PReLU(),\n",
    "                                         nn.Dropout(self.drop),\n",
    "                                         nn.Linear(self.hidden_dim *\n",
    "                                                   self.heads[0], in_feats)\n",
    "                                         ))\n",
    "\n",
    "        # build multiple layers\n",
    "        self.layers.append(TransformerConv(in_feats=self.in_feats,\n",
    "                                           out_feats=self.hidden_dim,\n",
    "                                           num_heads=self.heads[0],\n",
    "                                           skip_feat=skip_feat,\n",
    "                                           gated=gated,\n",
    "                                           layer_norm=layer_norm,\n",
    "                                           activation=self.activation))\n",
    "\n",
    "        for l in range(0, (self.n_layers - 1)):\n",
    "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
    "            self.layers.append(TransformerConv(in_feats=self.hidden_dim * self.heads[l - 1],\n",
    "                                               out_feats=self.hidden_dim,\n",
    "                                               num_heads=self.heads[l],\n",
    "                                               skip_feat=skip_feat,\n",
    "                                               gated=gated,\n",
    "                                               layer_norm=layer_norm,\n",
    "                                               activation=self.activation))\n",
    "        if post_proc:\n",
    "            self.layers.append(nn.Sequential(nn.Linear(self.hidden_dim * self.heads[-1], self.hidden_dim * self.heads[-1]),\n",
    "                                             nn.BatchNorm1d(\n",
    "                                                 self.hidden_dim * self.heads[-1]),\n",
    "                                             nn.PReLU(),\n",
    "                                             nn.Dropout(self.drop),\n",
    "                                             nn.Linear(self.hidden_dim * self.heads[-1], self.n_classes)))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(self.hidden_dim *\n",
    "                               self.heads[-1], self.n_classes))\n",
    "\n",
    "    def forward(self, blocks, features, labels, n2v_feat=None):\n",
    "        \"\"\"\n",
    "        :param blocks: train blocks\n",
    "        :param features: train features  (|input|, feta_dim)\n",
    "        :param labels: train labels (|input|, )\n",
    "        :param n2v_feat: whether to use n2v features \n",
    "        \"\"\"\n",
    "\n",
    "        if n2v_feat is None:\n",
    "            h = features\n",
    "        else:\n",
    "            h = self.n2v_mlp(n2v_feat)\n",
    "            h = features + h\n",
    "\n",
    "        label_embed = self.input_drop(self.layers[0](labels))\n",
    "        label_embed = self.layers[1](h) + self.layers[2](label_embed)\n",
    "        label_embed = self.layers[3](label_embed)\n",
    "        h = h + label_embed  # residual\n",
    "\n",
    "        for l in range(self.n_layers):\n",
    "            h = self.output_drop(self.layers[l+4](blocks[l], h))\n",
    "\n",
    "        logits = self.layers[-1](h)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "import pandas as pd\n",
    "\n",
    "# 模拟数据\n",
    "# 图数据\n",
    "g = dgl.rand_graph(100, 200)  # 生成一个包含100个节点和200条边的随机图\n",
    "# 节点特征\n",
    "num_features = 82\n",
    "features = torch.randn(g.num_nodes(), num_features)\n",
    "# 节点标签\n",
    "num_classes = 3\n",
    "labels = torch.randint(0, num_classes, (g.num_nodes(),))\n",
    "# 模拟参考数据\n",
    "ref_df = pd.DataFrame({\n",
    "    \"Target\": torch.randint(0, 5, (g.num_nodes(),)).tolist(),\n",
    "    \"Type\": torch.randint(0, 3, (g.num_nodes(),)).tolist(),\n",
    "    \"Location\": torch.randint(0, 4, (g.num_nodes(),)).tolist()\n",
    "})\n",
    "\n",
    "# 定义模型参数\n",
    "in_feats = num_features\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "heads = [4, 4]\n",
    "activation = nn.PReLU()\n",
    "skip_feat = True\n",
    "gated = True\n",
    "layer_norm = True\n",
    "post_proc = True\n",
    "n2v_feat = True\n",
    "drop = [0.2, 0.2]\n",
    "device = 'cpu'\n",
    "\n",
    "# 创建模型实例\n",
    "model = GraphAttnModel(\n",
    "    in_feats=in_feats,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_layers=n_layers,\n",
    "    n_classes=num_classes,\n",
    "    heads=heads,\n",
    "    activation=activation,\n",
    "    skip_feat=skip_feat,\n",
    "    gated=gated,\n",
    "    layer_norm=layer_norm,\n",
    "    post_proc=post_proc,\n",
    "    n2v_feat=n2v_feat,\n",
    "    drop=drop,\n",
    "    ref_df=ref_df,\n",
    "    cat_features=cat_features,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # 前向传播\n",
    "    logits = model([g], features, labels, ref_df)\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    # 反向传播和优化\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# 推理\n",
    "with torch.no_grad():\n",
    "    test_logits = model([g], features, labels, ref_df)\n",
    "    predicted_labels = torch.argmax(test_logits, dim=1)\n",
    "    print(\"Predicted labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "import pandas as pd\n",
    "\n",
    "# 模拟数据\n",
    "# 图数据\n",
    "g = dgl.rand_graph(100, 200)  # 生成一个包含100个节点和200条边的随机图\n",
    "# 节点特征\n",
    "num_features = 82\n",
    "features = torch.randn(g.num_nodes(), num_features)\n",
    "# 节点标签\n",
    "num_classes = 3\n",
    "labels = torch.randint(0, num_classes, (g.num_nodes(),))\n",
    "# 模拟参考数据\n",
    "ref_df = pd.DataFrame({\n",
    "    \"Target\": torch.randint(0, 5, (g.num_nodes(),)).tolist(),\n",
    "    \"Type\": torch.randint(0, 3, (g.num_nodes(),)).tolist(),\n",
    "    \"Location\": torch.randint(0, 4, (g.num_nodes(),)).tolist()\n",
    "})\n",
    "\n",
    "# 定义模型参数\n",
    "in_feats = num_features\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "heads = [4, 4]\n",
    "activation = nn.PReLU()\n",
    "skip_feat = True\n",
    "gated = True\n",
    "layer_norm = True\n",
    "post_proc = True\n",
    "n2v_feat = True\n",
    "drop = [0.2, 0.2]\n",
    "device = 'cpu'\n",
    "\n",
    "# 创建模型实例\n",
    "model = GraphAttnModel(\n",
    "    in_feats=in_feats,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_layers=n_layers,\n",
    "    n_classes=num_classes,\n",
    "    heads=heads,\n",
    "    activation=activation,\n",
    "    skip_feat=skip_feat,\n",
    "    gated=gated,\n",
    "    layer_norm=layer_norm,\n",
    "    post_proc=post_proc,\n",
    "    n2v_feat=n2v_feat,\n",
    "    drop=drop,\n",
    "    ref_df=ref_df,\n",
    "    cat_features=cat_features,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # 前向传播\n",
    "    logits = model([g], features, labels, ref_df)\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    # 反向传播和优化\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# 推理\n",
    "with torch.no_grad():\n",
    "    test_logits = model([g], features, labels, ref_df)\n",
    "    predicted_labels = torch.argmax(test_logits, dim=1)\n",
    "    print(\"Predicted labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cat_features = [\"Target\",\n",
    "                \"Type\",\n",
    "                \"Location\"]\n",
    "\n",
    "\n",
    "class PosEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, device, base=10000, bias=0):\n",
    "\n",
    "        super(PosEncoding, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize the posencoding component\n",
    "        :param dim: the encoding dimension \n",
    "        :param device: where to train model\n",
    "        :param base: the encoding base\n",
    "        :param bias: the encoding bias\n",
    "        \"\"\"\n",
    "        p = []\n",
    "        sft = []\n",
    "        for i in range(dim):\n",
    "            b = (i - i % 2) / dim\n",
    "            p.append(base ** -b)\n",
    "            if i % 2:\n",
    "                sft.append(np.pi / 2.0 + bias)\n",
    "            else:\n",
    "                sft.append(bias)\n",
    "        self.device = device\n",
    "        self.sft = torch.tensor(\n",
    "            sft, dtype=torch.float32).view(1, -1).to(device)\n",
    "        self.base = torch.tensor(p, dtype=torch.float32).view(1, -1).to(device)\n",
    "\n",
    "    def forward(self, pos):\n",
    "        with torch.no_grad():\n",
    "            if isinstance(pos, list):\n",
    "                pos = torch.tensor(pos, dtype=torch.float32).to(self.device)\n",
    "            pos = pos.view(-1, 1)\n",
    "            x = pos / self.base + self.sft\n",
    "            return torch.sin(x)\n",
    "\n",
    "\n",
    "class TransEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, df=None, device='cpu', dropout=0.2, in_feats=82, cat_features=None):\n",
    "        \"\"\"\n",
    "        Initialize the attribute embedding and feature learning compoent\n",
    "\n",
    "        :param df: the feature\n",
    "                :param device: where to train model\n",
    "                :param dropout: the dropout rate\n",
    "                :param in_feat: the shape of input feature in dimension 1\n",
    "                :param cat_feature: category features\n",
    "        \"\"\"\n",
    "        super(TransEmbedding, self).__init__()\n",
    "        self.time_pe = PosEncoding(dim=in_feats, device=device, base=100)\n",
    "        # time_emb = time_pe(torch.sin(torch.tensor(df['time_span'].values)/86400*torch.pi))\n",
    "        self.cat_table = nn.ModuleDict({col: nn.Embedding(max(df[col].unique(\n",
    "        ))+1, in_feats).to(device) for col in cat_features if col not in {\"Labels\", \"Time\"}})\n",
    "        self.label_table = nn.Embedding(3, in_feats, padding_idx=2).to(device)\n",
    "        self.time_emb = None\n",
    "        self.emb_dict = None\n",
    "        self.label_emb = None\n",
    "        self.cat_features = cat_features\n",
    "        self.forward_mlp = nn.ModuleList(\n",
    "            [nn.Linear(in_feats, in_feats) for i in range(len(cat_features))])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward_emb(self, df):\n",
    "        if self.emb_dict is None:\n",
    "            self.emb_dict = self.cat_table\n",
    "        # Convert pandas.Series to torch.Tensor\n",
    "        support = {col: self.emb_dict[col](torch.tensor(df[col].values, dtype=torch.long).to(self.emb_dict[col].weight.device))\n",
    "                   for col in self.cat_features if col not in {\"Labels\", \"Time\"}}\n",
    "        # self.time_emb = self.time_pe(torch.sin(torch.tensor(df['time_span'])/86400*torch.pi))\n",
    "        # support['time_span'] = self.time_emb\n",
    "        # support['labels'] = self.label_table(df['labels'])\n",
    "        return support\n",
    "\n",
    "    def forward(self, df):\n",
    "        support = self.forward_emb(df)\n",
    "        output = 0\n",
    "        for i, k in enumerate(support.keys()):\n",
    "            # if k =='time_span':\n",
    "            #    print(df[k].shape)\n",
    "            support[k] = self.dropout(support[k])\n",
    "            support[k] = self.forward_mlp[i](support[k])\n",
    "            output = output + support[k]\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerConv(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 allow_zero_in_degree=False,\n",
    "                 # feat_drop=0.6,\n",
    "                 # attn_drop=0.6,\n",
    "                 skip_feat=True,\n",
    "                 gated=True,\n",
    "                 layer_norm=True,\n",
    "                 activation=nn.PReLU()):\n",
    "        \"\"\"\n",
    "        Initialize the transformer layer.\n",
    "        Attentional weights are jointly optimized in an end-to-end mechanism with graph neural networks and fraud detection networks.\n",
    "            :param in_feat: the shape of input feature\n",
    "            :param out_feats: the shape of output feature\n",
    "            :param num_heads: the number of multi-head attention \n",
    "            :param bias: whether to use bias\n",
    "            :param allow_zero_in_degree: whether to allow zero in degree\n",
    "            :param skip_feat: whether to skip some feature \n",
    "            :param gated: whether to use gate\n",
    "            :param layer_norm: whether to use layer regularization\n",
    "            :param activation: the type of activation function   \n",
    "        \"\"\"\n",
    "\n",
    "        super(TransformerConv, self).__init__()\n",
    "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
    "        self._out_feats = out_feats\n",
    "        self._allow_zero_in_degree = allow_zero_in_degree\n",
    "        self._num_heads = num_heads\n",
    "\n",
    "        self.lin_query = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats*self._num_heads, bias=bias)\n",
    "        self.lin_key = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats*self._num_heads, bias=bias)\n",
    "        self.lin_value = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats*self._num_heads, bias=bias)\n",
    "\n",
    "        # self.feat_dropout = nn.Dropout(p=feat_drop)\n",
    "        # self.attn_dropout = nn.Dropout(p=attn_drop)\n",
    "        if skip_feat:\n",
    "            self.skip_feat = nn.Linear(\n",
    "                self._in_src_feats, self._out_feats*self._num_heads, bias=bias)\n",
    "        else:\n",
    "            self.skip_feat = None\n",
    "        if gated:\n",
    "            self.gate = nn.Linear(\n",
    "                3*self._out_feats*self._num_heads, 1, bias=bias)\n",
    "        else:\n",
    "            self.gate = None\n",
    "        if layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(self._out_feats*self._num_heads)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, graph, feat, get_attention=False):\n",
    "        \"\"\"\n",
    "        Description: Transformer Graph Convolution\n",
    "        :param graph: input graph\n",
    "            :param feat: input feat\n",
    "            :param get_attention: whether to get attention\n",
    "        \"\"\"\n",
    "\n",
    "        graph = graph.local_var()\n",
    "\n",
    "        if not self._allow_zero_in_degree:\n",
    "            if (graph.in_degrees() == 0).any():\n",
    "                raise DGLError('There are 0-in-degree nodes in the graph, '\n",
    "                               'output for those nodes will be invalid. '\n",
    "                               'This is harmful for some applications, '\n",
    "                               'causing silent performance regression. '\n",
    "                               'Adding self-loop on the input graph by '\n",
    "                               'calling `g = dgl.add_self_loop(g)` will resolve '\n",
    "                               'the issue. Setting ``allow_zero_in_degree`` '\n",
    "                               'to be `True` when constructing this module will '\n",
    "                               'suppress the check and let the code run.')\n",
    "\n",
    "        # check if feat is a tuple\n",
    "        if isinstance(feat, tuple):\n",
    "            h_src = feat[0]\n",
    "            h_dst = feat[1]\n",
    "        else:\n",
    "            h_src = feat\n",
    "            h_dst = h_src[:graph.number_of_dst_nodes()]\n",
    "\n",
    "        # Step 0. q, k, v\n",
    "        q_src = self.lin_query(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        k_dst = self.lin_key(h_dst).view(-1, self._num_heads, self._out_feats)\n",
    "        v_src = self.lin_value(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        # Assign features to nodes\n",
    "        graph.srcdata.update({'ft': q_src, 'ft_v': v_src})\n",
    "        graph.dstdata.update({'ft': k_dst})\n",
    "        # Step 1. dot product\n",
    "        graph.apply_edges(fn.u_dot_v('ft', 'ft', 'a'))\n",
    "\n",
    "        # Step 2. edge softmax to compute attention scores\n",
    "        graph.edata['sa'] = edge_softmax(\n",
    "            graph, graph.edata['a'] / self._out_feats**0.5)\n",
    "\n",
    "        # Step 3. Broadcast softmax value to each edge, and aggregate dst node\n",
    "        graph.update_all(fn.u_mul_e('ft_v', 'sa', 'attn'),\n",
    "                         fn.sum('attn', 'agg_u'))\n",
    "\n",
    "        # output results to the destination nodes\n",
    "        rst = graph.dstdata['agg_u'].reshape(-1,\n",
    "                                             self._out_feats*self._num_heads)\n",
    "\n",
    "        if self.skip_feat is not None:\n",
    "            skip_feat = self.skip_feat(feat[:graph.number_of_dst_nodes()])\n",
    "            if self.gate is not None:\n",
    "                gate = torch.sigmoid(\n",
    "                    self.gate(\n",
    "                        torch.concat([skip_feat, rst, skip_feat - rst], dim=-1)))\n",
    "                rst = gate * skip_feat + (1 - gate) * rst\n",
    "            else:\n",
    "                rst = skip_feat + rst\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            rst = self.layer_norm(rst)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            rst = self.activation(rst)\n",
    "\n",
    "        if get_attention:\n",
    "            return rst, graph.edata['sa']\n",
    "        else:\n",
    "            return rst\n",
    "\n",
    "\n",
    "class GraphAttnModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 hidden_dim,\n",
    "                 n_layers,\n",
    "                 n_classes,\n",
    "                 heads,\n",
    "                 activation,\n",
    "                 skip_feat=True,\n",
    "                 gated=True,\n",
    "                 layer_norm=True,\n",
    "                 post_proc=True,\n",
    "                 n2v_feat=True,\n",
    "                 drop=None,\n",
    "                 ref_df=None,\n",
    "                 cat_features=None,\n",
    "                 nei_features=None,\n",
    "                 device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the GTAN-GNN model \n",
    "        :param in_feats: the shape of input feature\n",
    "                :param hidden_dim: model hidden layer dimension\n",
    "                :param n_layers: the number of GTAN layers\n",
    "                :param n_classes: the number of classification\n",
    "                :param heads: the number of multi-head attention \n",
    "                :param activation: the type of activation function\n",
    "                :param skip_feat: whether to skip some feature\n",
    "                :param gated: whether to use gate\n",
    "        :param layer_norm: whether to use layer regularization\n",
    "                :param post_proc: whether to use post processing\n",
    "                :param n2v_feat: whether to use n2v features\n",
    "        :param drop: whether to use drop\n",
    "                :param ref_df: whether to refer other node features\n",
    "                :param cat_features: category features\n",
    "                :param nei_features: neighborhood statistic features\n",
    "        :param device: where to train model\n",
    "        \"\"\"\n",
    "\n",
    "        super(GraphAttnModel, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_classes = n_classes\n",
    "        self.heads = heads\n",
    "        self.activation = activation\n",
    "        # self.input_drop = lambda x: x\n",
    "        self.input_drop = nn.Dropout(drop[0])\n",
    "        self.drop = drop[1]\n",
    "        self.output_drop = nn.Dropout(self.drop)\n",
    "        # self.pn = PairNorm(mode=pairnorm)\n",
    "        if n2v_feat:\n",
    "            self.n2v_mlp = TransEmbedding(\n",
    "                ref_df, device=device, in_feats=in_feats, cat_features=cat_features)\n",
    "        else:\n",
    "            self.n2v_mlp = lambda x: x\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Embedding(\n",
    "            n_classes+1, in_feats, padding_idx=n_classes))\n",
    "        self.layers.append(\n",
    "            nn.Linear(self.in_feats, self.hidden_dim*self.heads[0]))\n",
    "        self.layers.append(\n",
    "            nn.Linear(self.in_feats, self.hidden_dim*self.heads[0]))\n",
    "        self.layers.append(nn.Sequential(nn.BatchNorm1d(self.hidden_dim*self.heads[0]),\n",
    "                                         nn.PReLU(),\n",
    "                                         nn.Dropout(self.drop),\n",
    "                                         nn.Linear(self.hidden_dim *\n",
    "                                                   self.heads[0], in_feats)\n",
    "                                         ))\n",
    "\n",
    "        # build multiple layers\n",
    "        self.layers.append(TransformerConv(in_feats=self.in_feats,\n",
    "                                           out_feats=self.hidden_dim,\n",
    "                                           num_heads=self.heads[0],\n",
    "                                           skip_feat=skip_feat,\n",
    "                                           gated=gated,\n",
    "                                           layer_norm=layer_norm,\n",
    "                                           activation=self.activation))\n",
    "\n",
    "        for l in range(0, (self.n_layers - 1)):\n",
    "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
    "            self.layers.append(TransformerConv(in_feats=self.hidden_dim * self.heads[l - 1],\n",
    "                                               out_feats=self.hidden_dim,\n",
    "                                               num_heads=self.heads[l],\n",
    "                                               skip_feat=skip_feat,\n",
    "                                               gated=gated,\n",
    "                                               layer_norm=layer_norm,\n",
    "                                               activation=self.activation))\n",
    "        if post_proc:\n",
    "            self.layers.append(nn.Sequential(nn.Linear(self.hidden_dim * self.heads[-1], self.hidden_dim * self.heads[-1]),\n",
    "                                             nn.BatchNorm1d(\n",
    "                                                 self.hidden_dim * self.heads[-1]),\n",
    "                                             nn.PReLU(),\n",
    "                                             nn.Dropout(self.drop),\n",
    "                                             nn.Linear(self.hidden_dim * self.heads[-1], self.n_classes)))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(self.hidden_dim *\n",
    "                               self.heads[-1], self.n_classes))\n",
    "\n",
    "    def forward(self, blocks, features, labels, n2v_feat=None):\n",
    "        \"\"\"\n",
    "        :param blocks: train blocks\n",
    "        :param features: train features  (|input|, feta_dim)\n",
    "        :param labels: train labels (|input|, )\n",
    "        :param n2v_feat: whether to use n2v features \n",
    "        \"\"\"\n",
    "\n",
    "        if n2v_feat is None:\n",
    "            h = features\n",
    "        else:\n",
    "            h = self.n2v_mlp(n2v_feat)\n",
    "            h = features + h\n",
    "\n",
    "        label_embed = self.input_drop(self.layers[0](labels))\n",
    "        label_embed = self.layers[1](h) + self.layers[2](label_embed)\n",
    "        label_embed = self.layers[3](label_embed)\n",
    "        h = h + label_embed  # residual\n",
    "\n",
    "        for l in range(self.n_layers):\n",
    "            h = self.output_drop(self.layers[l+4](blocks[l], h))\n",
    "\n",
    "        logits = self.layers[-1](h)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 1. 数据准备\n",
    "# 模拟图数据\n",
    "num_nodes = 100\n",
    "num_edges = 200\n",
    "g = dgl.rand_graph(num_nodes, num_edges)\n",
    "\n",
    "# 模拟节点特征\n",
    "in_feats = 82\n",
    "features = torch.randn(num_nodes, in_feats)\n",
    "\n",
    "# 模拟节点标签\n",
    "n_classes = 3\n",
    "labels = torch.randint(0, n_classes, (num_nodes,))\n",
    "\n",
    "# 模拟参考数据\n",
    "ref_df = pd.DataFrame({\n",
    "    \"Target\": torch.randint(0, 5, (num_nodes,)).tolist(),\n",
    "    \"Type\": torch.randint(0, 3, (num_nodes,)).tolist(),\n",
    "    \"Location\": torch.randint(0, 4, (num_nodes,)).tolist()\n",
    "})\n",
    "\n",
    "# 2. 模型初始化\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "heads = [4, 4]\n",
    "activation = nn.PReLU()\n",
    "skip_feat = True\n",
    "gated = True\n",
    "layer_norm = True\n",
    "post_proc = True\n",
    "n2v_feat = True\n",
    "drop = [0.2, 0.2]\n",
    "device = 'cpu'\n",
    "\n",
    "model = GraphAttnModel(\n",
    "    in_feats=in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 1. 数据准备\n",
    "# 模拟图数据\n",
    "num_nodes = 100\n",
    "num_edges = 200\n",
    "g = dgl.rand_graph(num_nodes, num_edges)\n",
    "\n",
    "# 模拟节点特征\n",
    "in_feats = 82\n",
    "features = torch.randn(num_nodes, in_feats)\n",
    "\n",
    "# 模拟节点标签\n",
    "n_classes = 3\n",
    "labels = torch.randint(0, n_classes, (num_nodes,))\n",
    "\n",
    "# 模拟参考数据\n",
    "ref_df = pd.DataFrame({\n",
    "    \"Target\": torch.randint(0, 5, (num_nodes,)).tolist(),\n",
    "    \"Type\": torch.randint(0, 3, (num_nodes,)).tolist(),\n",
    "    \"Location\": torch.randint(0, 4, (num_nodes,)).tolist()\n",
    "})\n",
    "\n",
    "# 将 ref_df 中的分类特征转换为 Tensor\n",
    "n2v_feat = torch.tensor(ref_df[cat_features].values, dtype=torch.long)\n",
    "\n",
    "# 2. 模型初始化\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "heads = [4, 4]\n",
    "activation = nn.PReLU()\n",
    "skip_feat = True\n",
    "gated = True\n",
    "layer_norm = True\n",
    "post_proc = True\n",
    "n2v_feat_flag = True  # 为了避免与 n2v_feat 变量名冲突\n",
    "drop = [0.2, 0.2]\n",
    "device = 'cpu'\n",
    "\n",
    "model = GraphAttnModel(\n",
    "    in_feats=in_feats,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_layers=n_layers,\n",
    "    n_classes=n_classes,\n",
    "    heads=heads,\n",
    "    activation=activation,\n",
    "    skip_feat=skip_feat,\n",
    "    gated=gated,\n",
    "    layer_norm=layer_norm,\n",
    "    post_proc=post_proc,\n",
    "    n2v_feat=n2v_feat_flag,\n",
    "    drop=drop,\n",
    "    ref_df=ref_df,\n",
    "    cat_features=cat_features,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 3. 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. 模型训练\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # 前向传播\n",
    "    blocks = [g]  # 这里简化为使用整个图作为一个 block\n",
    "    logits = model(blocks, features, labels, n2v_feat)\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    # 反向传播和优化\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# 5. 模型评估\n",
    "with torch.no_grad():\n",
    "    test_logits = model(blocks, features, labels, n2v_feat)\n",
    "    predicted_labels = torch.argmax(test_logits, dim=1)\n",
    "    accuracy = (predicted_labels == labels).float().mean()\n",
    "    print(f'Accuracy: {accuracy.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 模拟图结构 (1000笔交易)\n",
    "# ----------------------------\n",
    "num_nodes = 1000\n",
    "src_nodes = torch.randint(0, num_nodes, (5000,))  # 随机生成5000条交易边\n",
    "dst_nodes = torch.randint(0, num_nodes, (5000,))\n",
    "graph = dgl.graph((src_nodes, dst_nodes))\n",
    "\n",
    "# 构建2层采样子图块\n",
    "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n",
    "blocks = sampler.sample_blocks(graph, torch.tensor([0,1,2]))  # 示例采样\n",
    "\n",
    "# ----------------------------\n",
    "# 节点特征 (数值型特征)\n",
    "# ----------------------------\n",
    "features = torch.randn(num_nodes, 82)  # 82维原始特征\n",
    "\n",
    "# ----------------------------\n",
    "# 标签信息 (0-正常，1-欺诈，2-padding)\n",
    "# ----------------------------\n",
    "labels = torch.cat([\n",
    "    torch.zeros(800),   # 800正常交易\n",
    "    torch.ones(100),    # 100欺诈交易\n",
    "    torch.full((100,), 2)  # 100未标注\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 结构化特征DataFrame\n",
    "# ----------------------------\n",
    "n2v_df = pd.DataFrame({\n",
    "    'Target': np.random.randint(0, 5, num_nodes),  # 5种目标账户类型\n",
    "    'Type': np.random.randint(0, 3, num_nodes),    # 3种交易类型\n",
    "    'Location': np.random.randint(0, 10, num_nodes), # 10个地区编码\n",
    "    'time_span': np.random.uniform(0, 86400, num_nodes)  # 交易时间戳\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphAttnModel(\n",
    "    in_feats=82,\n",
    "    hidden_dim=128,\n",
    "    n_layers=2,\n",
    "    n_classes=2,\n",
    "    heads=[4,4],  # 每层4个头\n",
    "    activation=nn.PReLU(),\n",
    "    cat_features=cat_features,\n",
    "    device='cuda',\n",
    "    drop=[0.2,0.3]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "\n",
    "# 配置参数\n",
    "num_nodes = 1000\n",
    "in_feats = 82\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cat_features = [\"Target\", \"Type\", \"Location\"]\n",
    "\n",
    "# ----------------------------\n",
    "# 1. 数据准备（关键修正点）\n",
    "# ----------------------------\n",
    "# 生成完整的ref_df包含必须的类别列\n",
    "ref_df = pd.DataFrame({\n",
    "    'Target': np.random.randint(0, 5, num_nodes),\n",
    "    'Type': np.random.randint(0, 3, num_nodes),\n",
    "    'Location': np.random.randint(0, 10, num_nodes),\n",
    "    'time_span': np.random.uniform(0, 86400, num_nodes),\n",
    "    'labels': np.random.randint(0, 3, num_nodes)  # 添加labels列\n",
    "}).astype({'labels': 'category'})\n",
    "\n",
    "# ----------------------------\n",
    "# 2. 构建图结构\n",
    "# ----------------------------\n",
    "# 创建随机图\n",
    "src = torch.randint(0, num_nodes, (5000,))\n",
    "dst = torch.randint(0, num_nodes, (5000,))\n",
    "g = dgl.graph((src, dst)).to(device)\n",
    "\n",
    "# 构建2层采样子图\n",
    "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n",
    "seed_nodes = torch.arange(num_nodes).to(device)\n",
    "blocks = sampler.sample_blocks(g, seed_nodes)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. 初始化模型（关键参数设置）\n",
    "# ----------------------------\n",
    "model = GraphAttnModel(\n",
    "    in_feats=in_feats,\n",
    "    hidden_dim=64,\n",
    "    n_layers=2,\n",
    "    n_classes=2,\n",
    "    heads=[2, 2],\n",
    "    activation=nn.PReLU(),\n",
    "    n2v_feat=True,   # 明确启用n2v特征\n",
    "    ref_df=ref_df,   # 传递有效DataFrame\n",
    "    cat_features=cat_features,\n",
    "    device=device,\n",
    "    drop=[0.1, 0.2]\n",
    ").to(device)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. 准备输入数据\n",
    "# ----------------------------\n",
    "# 特征张量 (模拟数值特征)\n",
    "features = torch.randn(num_nodes, in_feats).to(device)\n",
    "\n",
    "# 标签数据 (包含padding索引2)\n",
    "labels = torch.cat([\n",
    "    torch.randint(0, 2, (800,)),   # 已标注数据\n",
    "    torch.full((200,), 2)          # 未标注数据\n",
    "]).to(device)\n",
    "\n",
    "# ----------------------------\n",
    "# 5. 执行前向传播\n",
    "# ----------------------------\n",
    "with torch.no_grad():\n",
    "    logits = model(blocks, features, labels, n2v_feat=ref_df)\n",
    "    print(\"输出logits尺寸:\", logits.shape)  # 应输出 torch.Size([1000, 2])\n",
    "\n",
    "# ----------------------------\n",
    "# 6. 验证关键组件\n",
    "# ----------------------------\n",
    "# 检查TransEmbedding输出\n",
    "trans_emb = model.n2v_mlp\n",
    "test_df = ref_df.iloc[:5]  # 取前5个样本测试\n",
    "emb_output = trans_emb(test_df)\n",
    "print(\"\\n特征增强示例:\")\n",
    "print(\"输入维度:\", test_df.shape)\n",
    "print(\"输出维度:\", emb_output.shape)  # 应输出 torch.Size([5, 82])\n",
    "\n",
    "# 检查TransformerConv层\n",
    "conv_layer = model.layers[4]\n",
    "h = torch.randn(g.num_nodes(), in_feats).to(device)\n",
    "out = conv_layer(g, h)\n",
    "print(\"\\n图注意力层输出尺寸:\", out.shape)  # 应输出 torch.Size([1000, 128])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7450714707374573, Train Acc: 0.48625001311302185, Test Acc: 0.47999998927116394\n",
      "Epoch 10, Loss: 0.6960091590881348, Train Acc: 0.5174999833106995, Test Acc: 0.49000000953674316\n",
      "Epoch 20, Loss: 0.6915732026100159, Train Acc: 0.5099999904632568, Test Acc: 0.45500001311302185\n",
      "Epoch 30, Loss: 0.6888428330421448, Train Acc: 0.5262500047683716, Test Acc: 0.5099999904632568\n",
      "Epoch 40, Loss: 0.6863663196563721, Train Acc: 0.543749988079071, Test Acc: 0.48500001430511475\n",
      "Epoch 50, Loss: 0.6836202144622803, Train Acc: 0.5362499952316284, Test Acc: 0.5249999761581421\n",
      "Epoch 60, Loss: 0.6804350018501282, Train Acc: 0.5462499856948853, Test Acc: 0.4950000047683716\n",
      "Epoch 70, Loss: 0.6767117381095886, Train Acc: 0.5487499833106995, Test Acc: 0.47999998927116394\n",
      "Epoch 80, Loss: 0.6723849773406982, Train Acc: 0.5550000071525574, Test Acc: 0.4699999988079071\n",
      "Epoch 90, Loss: 0.667324423789978, Train Acc: 0.5562499761581421, Test Acc: 0.48500001430511475\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "from dgl.dataloading import MultiLayerFullNeighborSampler, NodeDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 定义 TransformerConv 层\n",
    "class TransformerConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 allow_zero_in_degree=False,\n",
    "                 skip_feat=True,\n",
    "                 gated=True,\n",
    "                 layer_norm=True,\n",
    "                 activation=nn.PReLU()):\n",
    "        super(TransformerConv, self).__init__()\n",
    "        self._in_src_feats, self._in_dst_feats = in_feats, in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._allow_zero_in_degree = allow_zero_in_degree\n",
    "        self._num_heads = num_heads\n",
    "\n",
    "        self.lin_query = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_key = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_value = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "\n",
    "        if skip_feat:\n",
    "            self.skip_feat = nn.Linear(\n",
    "                self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        else:\n",
    "            self.skip_feat = None\n",
    "        if gated:\n",
    "            self.gate = nn.Linear(\n",
    "                3 * self._out_feats * self._num_heads, 1, bias=bias)\n",
    "        else:\n",
    "            self.gate = None\n",
    "        if layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(self._out_feats * self._num_heads)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, graph, feat, get_attention=False):\n",
    "        graph = graph.local_var()\n",
    "\n",
    "        if not self._allow_zero_in_degree:\n",
    "            if (graph.in_degrees() == 0).any():\n",
    "                raise ValueError('There are 0-in-degree nodes in the graph.')\n",
    "\n",
    "        if isinstance(feat, tuple):\n",
    "            h_src = feat[0]\n",
    "            h_dst = feat[1]\n",
    "        else:\n",
    "            h_src = feat\n",
    "            h_dst = h_src[:graph.number_of_dst_nodes()]\n",
    "\n",
    "        q_src = self.lin_query(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        k_dst = self.lin_key(h_dst).view(-1, self._num_heads, self._out_feats)\n",
    "        v_src = self.lin_value(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "\n",
    "        graph.srcdata.update({'ft': q_src, 'ft_v': v_src})\n",
    "        graph.dstdata.update({'ft': k_dst})\n",
    "\n",
    "        graph.apply_edges(dgl.function.u_dot_v('ft', 'ft', 'a'))\n",
    "        graph.edata['sa'] = dgl.nn.functional.edge_softmax(\n",
    "            graph, graph.edata['a'] / self._out_feats ** 0.5)\n",
    "\n",
    "        graph.update_all(dgl.function.u_mul_e('ft_v', 'sa', 'attn'),\n",
    "                         dgl.function.sum('attn', 'agg_u'))\n",
    "\n",
    "        rst = graph.dstdata['agg_u'].reshape(-1,\n",
    "                                             self._out_feats * self._num_heads)\n",
    "\n",
    "        if self.skip_feat is not None:\n",
    "            skip_feat = self.skip_feat(feat[:graph.number_of_dst_nodes()])\n",
    "            if self.gate is not None:\n",
    "                gate = torch.sigmoid(\n",
    "                    self.gate(\n",
    "                        torch.concat([skip_feat, rst, skip_feat - rst], dim=-1)))\n",
    "                rst = gate * skip_feat + (1 - gate) * rst\n",
    "            else:\n",
    "                rst = skip_feat + rst\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            rst = self.layer_norm(rst)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            rst = self.activation(rst)\n",
    "\n",
    "        if get_attention:\n",
    "            return rst, graph.edata['sa']\n",
    "        else:\n",
    "            return rst\n",
    "\n",
    "# 定义 GTAN 模型\n",
    "class GTAN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, n_layers, n_classes, heads):\n",
    "        super(GTAN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(TransformerConv(in_feats=in_feats,\n",
    "                                           out_feats=hidden_dim,\n",
    "                                           num_heads=heads[0]))\n",
    "        for l in range(0, (n_layers - 1)):\n",
    "            self.layers.append(TransformerConv(in_feats=hidden_dim * heads[l - 1],\n",
    "                                               out_feats=hidden_dim,\n",
    "                                               num_heads=heads[l]))\n",
    "        self.fc = nn.Linear(hidden_dim * heads[-1], n_classes)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        h = self.fc(h)\n",
    "        return h\n",
    "\n",
    "# 随机生成类似 FFSD 的数据集\n",
    "def generate_random_ffsd_data(num_nodes, num_features):\n",
    "    data = {\n",
    "        \"Source\": np.random.randint(0, 10, num_nodes),\n",
    "        \"Target\": np.random.randint(0, 10, num_nodes),\n",
    "        \"Location\": np.random.randint(0, 5, num_nodes),\n",
    "        \"Type\": np.random.randint(0, 3, num_nodes),\n",
    "        \"Amount\": np.random.randn(num_nodes),\n",
    "        \"Time\": np.random.randint(0, 100, num_nodes),\n",
    "        \"Labels\": np.random.randint(0, 2, num_nodes)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    cal_list = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "    for col in cal_list:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].apply(str).values)\n",
    "    feat_data = df.drop(\"Labels\", axis=1)\n",
    "    labels = df[\"Labels\"]\n",
    "    return feat_data, labels\n",
    "\n",
    "# 构建图\n",
    "def build_graph(feat_data):\n",
    "    alls = []\n",
    "    allt = []\n",
    "    pair = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "    for column in pair:\n",
    "        src, tgt = [], []\n",
    "        edge_per_trans = 3\n",
    "        for c_id, c_df in feat_data.groupby(column):\n",
    "            c_df = c_df.sort_values(by=\"Time\")\n",
    "            df_len = len(c_df)\n",
    "            sorted_idxs = c_df.index\n",
    "            src.extend([sorted_idxs[i] for i in range(df_len)\n",
    "                        for j in range(edge_per_trans) if i + j < df_len])\n",
    "            tgt.extend([sorted_idxs[i + j] for i in range(df_len)\n",
    "                        for j in range(edge_per_trans) if i + j < df_len])\n",
    "        alls.extend(src)\n",
    "        allt.extend(tgt)\n",
    "    alls = np.array(alls)\n",
    "    allt = np.array(allt)\n",
    "    g = dgl.graph((alls, allt))\n",
    "    return g\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, g, features, labels, train_idx, test_idx, epochs=100, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        logits = model(g, features)\n",
    "        loss = criterion(logits[train_idx], labels[train_idx])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(g, features)\n",
    "                train_preds = torch.argmax(logits[train_idx], dim=1)\n",
    "                test_preds = torch.argmax(logits[test_idx], dim=1)\n",
    "                train_acc = (train_preds == labels[train_idx]).float().mean()\n",
    "                test_acc = (test_preds == labels[test_idx]).float().mean()\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}, Train Acc: {train_acc.item()}, Test Acc: {test_acc.item()}')\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    num_nodes = 1000\n",
    "    num_features = 6\n",
    "    feat_data, labels = generate_random_ffsd_data(num_nodes, num_features) # 生成随机数据\n",
    "    g = build_graph(feat_data)\n",
    "\n",
    "    features = torch.from_numpy(feat_data.to_numpy()).float()\n",
    "    labels = torch.from_numpy(labels.to_numpy()).long()\n",
    "\n",
    "    train_idx, test_idx = train_test_split(np.arange(num_nodes), test_size=0.2, random_state=42)\n",
    "\n",
    "    in_feats = num_features\n",
    "    hidden_dim = 16\n",
    "    n_layers = 2\n",
    "    n_classes = 2\n",
    "    heads = [2, 2]\n",
    "\n",
    "    model = GTAN(in_feats, hidden_dim, n_layers, n_classes, heads)\n",
    "    train_model(model, g, features, labels, train_idx, test_idx)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.753218412399292, Train Acc: 0.5099999904632568, Test Acc: 0.47999998927116394\n",
      "Epoch 10, Loss: 0.695364236831665, Train Acc: 0.5199999809265137, Test Acc: 0.5699999928474426\n",
      "Epoch 20, Loss: 0.686575710773468, Train Acc: 0.5099999904632568, Test Acc: 0.47999998927116394\n",
      "Epoch 30, Loss: 0.6851438283920288, Train Acc: 0.5299999713897705, Test Acc: 0.5799999833106995\n",
      "Epoch 40, Loss: 0.6843498349189758, Train Acc: 0.5350000262260437, Test Acc: 0.5849999785423279\n",
      "Epoch 50, Loss: 0.6827506422996521, Train Acc: 0.53125, Test Acc: 0.5799999833106995\n",
      "Epoch 60, Loss: 0.680972695350647, Train Acc: 0.5400000214576721, Test Acc: 0.5799999833106995\n",
      "Epoch 70, Loss: 0.6784475445747375, Train Acc: 0.5475000143051147, Test Acc: 0.5849999785423279\n",
      "Epoch 80, Loss: 0.6747417449951172, Train Acc: 0.5375000238418579, Test Acc: 0.5649999976158142\n",
      "Epoch 90, Loss: 0.6698616743087769, Train Acc: 0.550000011920929, Test Acc: 0.5600000023841858\n",
      "预测结果: [0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1\n",
      " 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1\n",
      " 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "from dgl.dataloading import MultiLayerFullNeighborSampler, NodeDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 定义 TransformerConv 层\n",
    "class TransformerConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 allow_zero_in_degree=False,\n",
    "                 skip_feat=True,\n",
    "                 gated=True,\n",
    "                 layer_norm=True,\n",
    "                 activation=nn.PReLU()):\n",
    "        super(TransformerConv, self).__init__()\n",
    "        self._in_src_feats, self._in_dst_feats = in_feats, in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._allow_zero_in_degree = allow_zero_in_degree\n",
    "        self._num_heads = num_heads\n",
    "\n",
    "        self.lin_query = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_key = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_value = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "\n",
    "        if skip_feat:\n",
    "            self.skip_feat = nn.Linear(\n",
    "                self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        else:\n",
    "            self.skip_feat = None\n",
    "        if gated:\n",
    "            self.gate = nn.Linear(\n",
    "                3 * self._out_feats * self._num_heads, 1, bias=bias)\n",
    "        else:\n",
    "            self.gate = None\n",
    "        if layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(self._out_feats * self._num_heads)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, graph, feat, get_attention=False):\n",
    "        graph = graph.local_var()\n",
    "\n",
    "        if not self._allow_zero_in_degree:\n",
    "            if (graph.in_degrees() == 0).any():\n",
    "                raise ValueError('There are 0-in-degree nodes in the graph.')\n",
    "\n",
    "        if isinstance(feat, tuple):\n",
    "            h_src = feat[0]\n",
    "            h_dst = feat[1]\n",
    "        else:\n",
    "            h_src = feat\n",
    "            h_dst = h_src[:graph.number_of_dst_nodes()]\n",
    "\n",
    "        q_src = self.lin_query(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        k_dst = self.lin_key(h_dst).view(-1, self._num_heads, self._out_feats)\n",
    "        v_src = self.lin_value(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "\n",
    "        graph.srcdata.update({'ft': q_src, 'ft_v': v_src})\n",
    "        graph.dstdata.update({'ft': k_dst})\n",
    "\n",
    "        graph.apply_edges(dgl.function.u_dot_v('ft', 'ft', 'a'))\n",
    "        graph.edata['sa'] = dgl.nn.functional.edge_softmax(\n",
    "            graph, graph.edata['a'] / self._out_feats ** 0.5)\n",
    "\n",
    "        graph.update_all(dgl.function.u_mul_e('ft_v', 'sa', 'attn'),\n",
    "                         dgl.function.sum('attn', 'agg_u'))\n",
    "\n",
    "        rst = graph.dstdata['agg_u'].reshape(-1,\n",
    "                                             self._out_feats * self._num_heads)\n",
    "\n",
    "        if self.skip_feat is not None:\n",
    "            skip_feat = self.skip_feat(feat[:graph.number_of_dst_nodes()])\n",
    "            if self.gate is not None:\n",
    "                gate = torch.sigmoid(\n",
    "                    self.gate(\n",
    "                        torch.concat([skip_feat, rst, skip_feat - rst], dim=-1)))\n",
    "                rst = gate * skip_feat + (1 - gate) * rst\n",
    "            else:\n",
    "                rst = skip_feat + rst\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            rst = self.layer_norm(rst)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            rst = self.activation(rst)\n",
    "\n",
    "        if get_attention:\n",
    "            return rst, graph.edata['sa']\n",
    "        else:\n",
    "            return rst\n",
    "\n",
    "# 定义 GTAN 模型\n",
    "class GTAN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, n_layers, n_classes, heads):\n",
    "        super(GTAN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(TransformerConv(in_feats=in_feats,\n",
    "                                           out_feats=hidden_dim,\n",
    "                                           num_heads=heads[0]))\n",
    "        for l in range(0, (n_layers - 1)):\n",
    "            self.layers.append(TransformerConv(in_feats=hidden_dim * heads[l - 1],\n",
    "                                               out_feats=hidden_dim,\n",
    "                                               num_heads=heads[l]))\n",
    "        self.fc = nn.Linear(hidden_dim * heads[-1], n_classes)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        h = self.fc(h)\n",
    "        return h\n",
    "\n",
    "# 随机生成类似 FFSD 的数据集\n",
    "def generate_random_ffsd_data(num_nodes, num_features):\n",
    "    data = {\n",
    "        \"Source\": np.random.randint(0, 10, num_nodes),\n",
    "        \"Target\": np.random.randint(0, 10, num_nodes),\n",
    "        \"Location\": np.random.randint(0, 5, num_nodes),\n",
    "        \"Type\": np.random.randint(0, 3, num_nodes),\n",
    "        \"Amount\": np.random.randn(num_nodes),\n",
    "        \"Time\": np.random.randint(0, 100, num_nodes),\n",
    "        \"Labels\": np.random.randint(0, 2, num_nodes)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    cal_list = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "    for col in cal_list:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].apply(str).values)\n",
    "    feat_data = df.drop(\"Labels\", axis=1)\n",
    "    labels = df[\"Labels\"]\n",
    "    return feat_data, labels\n",
    "\n",
    "# 构建图\n",
    "def build_graph(feat_data):\n",
    "    alls = []\n",
    "    allt = []\n",
    "    pair = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "    for column in pair:\n",
    "        src, tgt = [], []\n",
    "        edge_per_trans = 3\n",
    "        for c_id, c_df in feat_data.groupby(column):\n",
    "            c_df = c_df.sort_values(by=\"Time\")\n",
    "            df_len = len(c_df)\n",
    "            sorted_idxs = c_df.index\n",
    "            src.extend([sorted_idxs[i] for i in range(df_len)\n",
    "                        for j in range(edge_per_trans) if i + j < df_len])\n",
    "            tgt.extend([sorted_idxs[i + j] for i in range(df_len)\n",
    "                        for j in range(edge_per_trans) if i + j < df_len])\n",
    "        alls.extend(src)\n",
    "        allt.extend(tgt)\n",
    "    alls = np.array(alls)\n",
    "    allt = np.array(allt)\n",
    "    g = dgl.graph((alls, allt))\n",
    "    return g\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, g, features, labels, train_idx, test_idx, epochs=100, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        logits = model(g, features)\n",
    "        loss = criterion(logits[train_idx], labels[train_idx])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(g, features)\n",
    "                train_preds = torch.argmax(logits[train_idx], dim=1)\n",
    "                test_preds = torch.argmax(logits[test_idx], dim=1)\n",
    "                train_acc = (train_preds == labels[train_idx]).float().mean()\n",
    "                test_acc = (test_preds == labels[test_idx]).float().mean()\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}, Train Acc: {train_acc.item()}, Test Acc: {test_acc.item()}')\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    num_nodes = 1000\n",
    "    num_features = 6\n",
    "    feat_data, labels = generate_random_ffsd_data(num_nodes, num_features)\n",
    "    g = build_graph(feat_data)\n",
    "\n",
    "    features = torch.from_numpy(feat_data.to_numpy()).float()\n",
    "    labels = torch.from_numpy(labels.to_numpy()).long()\n",
    "\n",
    "    train_idx, test_idx = train_test_split(np.arange(num_nodes), test_size=0.2, random_state=42)\n",
    "\n",
    "    in_feats = num_features\n",
    "    hidden_dim = 16\n",
    "    n_layers = 2\n",
    "    n_classes = 2\n",
    "    heads = [2, 2]\n",
    "\n",
    "    model = GTAN(in_feats, hidden_dim, n_layers, n_classes, heads)\n",
    "    train_model(model, g, features, labels, train_idx, test_idx)\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), 'gtan_model.pth')\n",
    "\n",
    "    # 模拟新数据\n",
    "    new_num_nodes = 100\n",
    "    new_feat_data, _ = generate_random_ffsd_data(new_num_nodes, num_features)\n",
    "    new_g = build_graph(new_feat_data)\n",
    "    new_features = torch.from_numpy(new_feat_data.to_numpy()).float()\n",
    "\n",
    "    # 加载模型\n",
    "    loaded_model = GTAN(in_feats, hidden_dim, n_layers, n_classes, heads)\n",
    "    loaded_model.load_state_dict(torch.load('gtan_model.pth'))\n",
    "    loaded_model.eval()\n",
    "\n",
    "    # 进行预测\n",
    "    with torch.no_grad():\n",
    "        logits = loaded_model(new_g, new_features)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        print(\"预测结果:\", predictions.numpy())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7000607252120972, Train Acc: 0.5024999976158142, Test Acc: 0.49000000953674316\n",
      "Epoch 10, Loss: 0.6918095350265503, Train Acc: 0.5412499904632568, Test Acc: 0.5\n",
      "Epoch 20, Loss: 0.6888192892074585, Train Acc: 0.5462499856948853, Test Acc: 0.4699999988079071\n",
      "Epoch 30, Loss: 0.6870434284210205, Train Acc: 0.5337499976158142, Test Acc: 0.4749999940395355\n",
      "Epoch 40, Loss: 0.6836686730384827, Train Acc: 0.5512499809265137, Test Acc: 0.5\n",
      "Epoch 50, Loss: 0.6786544322967529, Train Acc: 0.5649999976158142, Test Acc: 0.5149999856948853\n",
      "Epoch 60, Loss: 0.6714608669281006, Train Acc: 0.5737500190734863, Test Acc: 0.5249999761581421\n",
      "Epoch 70, Loss: 0.6621057391166687, Train Acc: 0.5950000286102295, Test Acc: 0.5299999713897705\n",
      "Epoch 80, Loss: 0.6528637409210205, Train Acc: 0.5962499976158142, Test Acc: 0.5149999856948853\n",
      "Epoch 90, Loss: 0.6451188921928406, Train Acc: 0.6137499809265137, Test Acc: 0.5199999809265137\n",
      "预测结果: [1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0\n",
      " 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0\n",
      " 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "from dgl.dataloading import MultiLayerFullNeighborSampler, NodeDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 定义 TransformerConv 层\n",
    "class TransformerConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,  # 输入特征的维度\n",
    "                 out_feats,  # 输出特征的维度\n",
    "                 num_heads,  # 注意力头的数量\n",
    "                 bias=True,  # 是否使用偏置项\n",
    "                 allow_zero_in_degree=False,  # 是否允许图中存在入度为 0 的节点\n",
    "                 skip_feat=True,  # 是否使用跳跃连接\n",
    "                 gated=True,  # 是否使用门控机制\n",
    "                 layer_norm=True,  # 是否使用层归一化\n",
    "                 activation=nn.PReLU()):  # 激活函数\n",
    "        super(TransformerConv, self).__init__()\n",
    "        # 输入源节点和目标节点的特征维度\n",
    "        self._in_src_feats, self._in_dst_feats = in_feats, in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._allow_zero_in_degree = allow_zero_in_degree\n",
    "        self._num_heads = num_heads\n",
    "\n",
    "        # 定义线性层，用于计算查询（query）、键（key）和值（value）\n",
    "        self.lin_query = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_key = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_value = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "\n",
    "        # 如果使用跳跃连接，定义线性层\n",
    "        if skip_feat:\n",
    "            self.skip_feat = nn.Linear(\n",
    "                self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        else:\n",
    "            self.skip_feat = None\n",
    "        # 如果使用门控机制，定义线性层\n",
    "        if gated:\n",
    "            self.gate = nn.Linear(\n",
    "                3 * self._out_feats * self._num_heads, 1, bias=bias)\n",
    "        else:\n",
    "            self.gate = None\n",
    "        # 如果使用层归一化，定义层归一化层\n",
    "        if layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(self._out_feats * self._num_heads)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, graph, feat, get_attention=False):\n",
    "        # 创建图的本地副本\n",
    "        graph = graph.local_var()\n",
    "\n",
    "        # 如果不允许存在入度为 0 的节点，检查图中是否存在此类节点\n",
    "        if not self._allow_zero_in_degree:\n",
    "            if (graph.in_degrees() == 0).any():\n",
    "                raise ValueError('There are 0-in-degree nodes in the graph.')\n",
    "\n",
    "        # 如果输入特征是元组，分别获取源节点和目标节点的特征\n",
    "        if isinstance(feat, tuple):\n",
    "            h_src = feat[0]\n",
    "            h_dst = feat[1]\n",
    "        else:\n",
    "            h_src = feat\n",
    "            h_dst = h_src[:graph.number_of_dst_nodes()]\n",
    "\n",
    "        # 计算查询、键和值\n",
    "        q_src = self.lin_query(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        k_dst = self.lin_key(h_dst).view(-1, self._num_heads, self._out_feats)\n",
    "        v_src = self.lin_value(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "\n",
    "        # 将查询和值存储在源节点数据中，将键存储在目标节点数据中\n",
    "        graph.srcdata.update({'ft': q_src, 'ft_v': v_src})\n",
    "        graph.dstdata.update({'ft': k_dst})\n",
    "\n",
    "        # 计算边的注意力分数\n",
    "        graph.apply_edges(dgl.function.u_dot_v('ft', 'ft', 'a'))\n",
    "        # 对注意力分数进行归一化\n",
    "        graph.edata['sa'] = dgl.nn.functional.edge_softmax(\n",
    "            graph, graph.edata['a'] / self._out_feats ** 0.5)\n",
    "\n",
    "        # 聚合邻居节点的特征\n",
    "        graph.update_all(dgl.function.u_mul_e('ft_v', 'sa', 'attn'),\n",
    "                         dgl.function.sum('attn', 'agg_u'))\n",
    "\n",
    "        # 重塑聚合后的特征\n",
    "        rst = graph.dstdata['agg_u'].reshape(-1,\n",
    "                                             self._out_feats * self._num_heads)\n",
    "\n",
    "        # 如果使用跳跃连接\n",
    "        if self.skip_feat is not None:\n",
    "            skip_feat = self.skip_feat(feat[:graph.number_of_dst_nodes()])\n",
    "            # 如果使用门控机制\n",
    "            if self.gate is not None:\n",
    "                # 计算门控值\n",
    "                gate = torch.sigmoid(\n",
    "                    self.gate(\n",
    "                        torch.concat([skip_feat, rst, skip_feat - rst], dim=-1)))\n",
    "                # 结合跳跃连接和聚合后的特征\n",
    "                rst = gate * skip_feat + (1 - gate) * rst\n",
    "            else:\n",
    "                rst = skip_feat + rst\n",
    "\n",
    "        # 如果使用层归一化\n",
    "        if self.layer_norm is not None:\n",
    "            rst = self.layer_norm(rst)\n",
    "\n",
    "        # 如果使用激活函数\n",
    "        if self.activation is not None:\n",
    "            rst = self.activation(rst)\n",
    "\n",
    "        # 如果需要返回注意力分数\n",
    "        if get_attention:\n",
    "            return rst, graph.edata['sa']\n",
    "        else:\n",
    "            return rst\n",
    "\n",
    "# 定义 GTAN 模型\n",
    "class GTAN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, n_layers, n_classes, heads):\n",
    "        super(GTAN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # 添加第一层 TransformerConv 层\n",
    "        self.layers.append(TransformerConv(in_feats=in_feats,\n",
    "                                           out_feats=hidden_dim,\n",
    "                                           num_heads=heads[0]))\n",
    "        # 添加后续的 TransformerConv 层\n",
    "        for l in range(0, (n_layers - 1)):\n",
    "            self.layers.append(TransformerConv(in_feats=hidden_dim * heads[l - 1],\n",
    "                                               out_feats=hidden_dim,\n",
    "                                               num_heads=heads[l]))\n",
    "        # 定义全连接层，用于输出分类结果\n",
    "        self.fc = nn.Linear(hidden_dim * heads[-1], n_classes)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        # 依次通过 TransformerConv 层\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        # 通过全连接层\n",
    "        h = self.fc(h)\n",
    "        return h\n",
    "\n",
    "# 随机生成类似 FFSD 的数据集\n",
    "def generate_random_ffsd_data(num_nodes, num_features):\n",
    "    data = {\n",
    "        \"Source\": np.random.randint(0, 10, num_nodes),\n",
    "        \"Target\": np.random.randint(0, 10, num_nodes),\n",
    "        \"Location\": np.random.randint(0, 5, num_nodes),\n",
    "        \"Type\": np.random.randint(0, 3, num_nodes),\n",
    "        \"Amount\": np.random.randn(num_nodes),\n",
    "        \"Time\": np.random.randint(0, 100, num_nodes),\n",
    "        \"Labels\": np.random.randint(0, 2, num_nodes)\n",
    "    }\n",
    "    # 创建 DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    cal_list = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "    # 对分类特征进行编码\n",
    "    for col in cal_list:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].apply(str).values)\n",
    "    # 提取特征数据\n",
    "    feat_data = df.drop(\"Labels\", axis=1)\n",
    "    # 提取标签数据\n",
    "    labels = df[\"Labels\"]\n",
    "    return feat_data, labels\n",
    "\n",
    "# 构建图\n",
    "def build_graph(feat_data):\n",
    "    alls = []\n",
    "    allt = []\n",
    "    pair = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "    for column in pair:\n",
    "        src, tgt = [], []\n",
    "        edge_per_trans = 3\n",
    "        # 按列进行分组\n",
    "        for c_id, c_df in feat_data.groupby(column):\n",
    "            # 按时间排序\n",
    "            c_df = c_df.sort_values(by=\"Time\")\n",
    "            df_len = len(c_df)\n",
    "            sorted_idxs = c_df.index\n",
    "            # 构建边\n",
    "            src.extend([sorted_idxs[i] for i in range(df_len)\n",
    "                        for j in range(edge_per_trans) if i + j < df_len])\n",
    "            tgt.extend([sorted_idxs[i + j] for i in range(df_len)\n",
    "                        for j in range(edge_per_trans) if i + j < df_len])\n",
    "        alls.extend(src)\n",
    "        allt.extend(tgt)\n",
    "    alls = np.array(alls)\n",
    "    allt = np.array(allt)\n",
    "    # 创建图\n",
    "    g = dgl.graph((alls, allt))\n",
    "    return g\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, g, features, labels, train_idx, test_idx, epochs=100, lr=0.001):\n",
    "    # 定义优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # 定义损失函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 模型训练模式\n",
    "        model.train()\n",
    "        # 前向传播\n",
    "        logits = model(g, features)\n",
    "        # 计算损失\n",
    "        loss = criterion(logits[train_idx], labels[train_idx])\n",
    "\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每 10 个 epoch 输出一次训练和测试准确率\n",
    "        if epoch % 10 == 0:\n",
    "            # 模型评估模式\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(g, features)\n",
    "                # 计算训练集预测结果\n",
    "                train_preds = torch.argmax(logits[train_idx], dim=1)\n",
    "                # 计算测试集预测结果\n",
    "                test_preds = torch.argmax(logits[test_idx], dim=1)\n",
    "                # 计算训练集准确率\n",
    "                train_acc = (train_preds == labels[train_idx]).float().mean()\n",
    "                # 计算测试集准确率\n",
    "                test_acc = (test_preds == labels[test_idx]).float().mean()\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}, Train Acc: {train_acc.item()}, Test Acc: {test_acc.item()}')\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    num_nodes = 1000\n",
    "    num_features = 6\n",
    "    # 生成随机数据集\n",
    "    feat_data, labels = generate_random_ffsd_data(num_nodes, num_features)\n",
    "    # 构建图\n",
    "    g = build_graph(feat_data)\n",
    "\n",
    "    # 将特征数据转换为 PyTorch 张量\n",
    "    features = torch.from_numpy(feat_data.to_numpy()).float()\n",
    "    # 将标签数据转换为 PyTorch 张量\n",
    "    labels = torch.from_numpy(labels.to_numpy()).long()\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    train_idx, test_idx = train_test_split(np.arange(num_nodes), test_size=0.2, random_state=42)\n",
    "\n",
    "    in_feats = num_features # 输入特征的维度\n",
    "    hidden_dim = 16   # 隐藏层的维度\n",
    "    n_layers = 2 # TransformerConv 层的数量\n",
    "    n_classes = 2 #      输出的类别数\n",
    "    heads = [2, 2] # 每个 TransformerConv 层的注意力头的数量\n",
    "\n",
    "    # 创建 GTAN 模型\n",
    "    model = GTAN(in_feats, hidden_dim, n_layers, n_classes, heads)\n",
    "    # 训练模型\n",
    "    train_model(model, g, features, labels, train_idx, test_idx)\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), 'gtan_model.pth')\n",
    "\n",
    "    # 模拟新数据\n",
    "    new_num_nodes = 100\n",
    "    new_feat_data, _ = generate_random_ffsd_data(new_num_nodes, num_features)\n",
    "    # 构建新图\n",
    "    new_g = build_graph(new_feat_data)\n",
    "    # 将新特征数据转换为 PyTorch 张量\n",
    "    new_features = torch.from_numpy(new_feat_data.to_numpy()).float()\n",
    "\n",
    "    # 加载模型\n",
    "    loaded_model = GTAN(in_feats, hidden_dim, n_layers, n_classes, heads)\n",
    "    loaded_model.load_state_dict(torch.load('gtan_model.pth'))\n",
    "    # 模型评估模式\n",
    "    loaded_model.eval()\n",
    "\n",
    "    # 进行预测\n",
    "    with torch.no_grad():\n",
    "        logits = loaded_model(new_g, new_features)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        print(\"预测结果:\", predictions.numpy())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_ffsd_data(num_nodes, num_features):\n",
    "    data = {\n",
    "        \"Source\": np.random.randint(0, 10, num_nodes),\n",
    "        \"Target\": np.random.randint(0, 10, num_nodes),\n",
    "        \"Location\": np.random.randint(0, 5, num_nodes),\n",
    "        \"Type\": np.random.randint(0, 3, num_nodes),\n",
    "        \"Amount\": np.random.randn(num_nodes),\n",
    "        \"Time\": np.random.randint(0, 100, num_nodes),\n",
    "        \"Labels\": np.random.randint(0, 2, num_nodes)\n",
    "    }\n",
    "    # 创建 DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    cal_list = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "    # 对分类特征进行编码\n",
    "    for col in cal_list:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].apply(str).values)\n",
    "    # 提取特征数据\n",
    "    feat_data = df.drop(\"Labels\", axis=1)\n",
    "    # 提取标签数据\n",
    "    labels = df[\"Labels\"]\n",
    "    return feat_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6)\n",
      "   Source  Target  Location  Type    Amount  Time\n",
      "0       0       4         3     2 -0.514793    17\n",
      "1       3       5         0     0 -0.439897    86\n",
      "2       8       1         0     0  0.536507    24\n",
      "3       2       9         0     0 -0.176634    97\n",
      "4       0       1         1     1 -0.843627    73\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 1000\n",
    "num_features = 6\n",
    "# 生成随机数据集\n",
    "feat_data, labels = generate_random_ffsd_data(num_nodes, num_features)\n",
    "print(feat_data.shape)\n",
    "print(feat_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建图\n",
    "def build_graph(feat_data):\n",
    "    alls = []\n",
    "    allt = []\n",
    "    pair = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "    for column in pair:\n",
    "        src, tgt = [], []\n",
    "        edge_per_trans = 3\n",
    "        # 按列进行分组\n",
    "        for c_id, c_df in feat_data.groupby(column):\n",
    "            # 按时间排序\n",
    "            c_df = c_df.sort_values(by=\"Time\")\n",
    "            df_len = len(c_df)\n",
    "            sorted_idxs = c_df.index\n",
    "            # 构建边\n",
    "            src.extend([sorted_idxs[i] for i in range(df_len)\n",
    "                        for j in range(edge_per_trans) if i + j < df_len])\n",
    "            tgt.extend([sorted_idxs[i + j] for i in range(df_len)\n",
    "                        for j in range(edge_per_trans) if i + j < df_len])\n",
    "        alls.extend(src)\n",
    "        allt.extend(tgt)\n",
    "    alls = np.array(alls)\n",
    "    allt = np.array(allt)\n",
    "    # 创建图\n",
    "    g = dgl.graph((alls, allt))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = build_graph(feat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 将特征数据转换为 PyTorch 张量\n",
    "features = torch.from_numpy(feat_data.to_numpy()).float()\n",
    "    # 将标签数据转换为 PyTorch 张量\n",
    "labels = torch.from_numpy(labels.to_numpy()).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = train_test_split(np.arange(num_nodes), test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feats = num_features # 输入特征的维度\n",
    "hidden_dim = 16   # 隐藏层的维度\n",
    "n_layers = 2 # TransformerConv 层的数量\n",
    "n_classes = 2 #      输出的类别数\n",
    "heads = [2, 2] # 每个 TransformerConv 层的注意力头的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 GTAN 模型\n",
    "class GTAN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, n_layers, n_classes, heads):\n",
    "        super(GTAN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # 添加第一层 TransformerConv 层\n",
    "        self.layers.append(TransformerConv(in_feats=in_feats,\n",
    "                                           out_feats=hidden_dim,\n",
    "                                           num_heads=heads[0]))\n",
    "        # 添加后续的 TransformerConv 层\n",
    "        for l in range(0, (n_layers - 1)):\n",
    "            self.layers.append(TransformerConv(in_feats=hidden_dim * heads[l - 1],\n",
    "                                               out_feats=hidden_dim,\n",
    "                                               num_heads=heads[l]))\n",
    "        # 定义全连接层，用于输出分类结果\n",
    "        self.fc = nn.Linear(hidden_dim * heads[-1], n_classes)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        # 依次通过 TransformerConv 层\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        # 通过全连接层\n",
    "        h = self.fc(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 TransformerConv 层\n",
    "class TransformerConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,  # 输入特征的维度\n",
    "                 out_feats,  # 输出特征的维度\n",
    "                 num_heads,  # 注意力头的数量\n",
    "                 bias=True,  # 是否使用偏置项\n",
    "                 allow_zero_in_degree=False,  # 是否允许图中存在入度为 0 的节点\n",
    "                 skip_feat=True,  # 是否使用跳跃连接\n",
    "                 gated=True,  # 是否使用门控机制\n",
    "                 layer_norm=True,  # 是否使用层归一化\n",
    "                 activation=nn.PReLU()):  # 激活函数\n",
    "        super(TransformerConv, self).__init__()\n",
    "        # 输入源节点和目标节点的特征维度\n",
    "        self._in_src_feats, self._in_dst_feats = in_feats, in_feats \n",
    "        self._out_feats = out_feats\n",
    "        self._allow_zero_in_degree = allow_zero_in_degree\n",
    "        self._num_heads = num_heads\n",
    "\n",
    "        # 定义线性层，用于计算查询（query）、键（key）和值（value）\n",
    "        self.lin_query = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_key = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_value = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "\n",
    "        # 如果使用跳跃连接，定义线性层\n",
    "        if skip_feat:\n",
    "            self.skip_feat = nn.Linear(\n",
    "                self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        else:\n",
    "            self.skip_feat = None\n",
    "        # 如果使用门控机制，定义线性层\n",
    "        if gated:\n",
    "            self.gate = nn.Linear(\n",
    "                3 * self._out_feats * self._num_heads, 1, bias=bias)\n",
    "        else:\n",
    "            self.gate = None\n",
    "        # 如果使用层归一化，定义层归一化层\n",
    "        if layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(self._out_feats * self._num_heads)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, graph, feat, get_attention=False):\n",
    "        # 创建图的本地副本\n",
    "        graph = graph.local_var()\n",
    "\n",
    "        # 如果不允许存在入度为 0 的节点，检查图中是否存在此类节点\n",
    "        if not self._allow_zero_in_degree:\n",
    "            if (graph.in_degrees() == 0).any():\n",
    "                raise ValueError('There are 0-in-degree nodes in the graph.')\n",
    "\n",
    "        # 如果输入特征是元组，分别获取源节点和目标节点的特征\n",
    "        if isinstance(feat, tuple):\n",
    "            h_src = feat[0]\n",
    "            h_dst = feat[1]\n",
    "        else:\n",
    "            h_src = feat\n",
    "            h_dst = h_src[:graph.number_of_dst_nodes()]\n",
    "\n",
    "        # 计算查询、键和值\n",
    "        q_src = self.lin_query(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        k_dst = self.lin_key(h_dst).view(-1, self._num_heads, self._out_feats)\n",
    "        v_src = self.lin_value(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "\n",
    "        # 将查询和值存储在源节点数据中，将键存储在目标节点数据中\n",
    "        graph.srcdata.update({'ft': q_src, 'ft_v': v_src})\n",
    "        graph.dstdata.update({'ft': k_dst})\n",
    "\n",
    "        # 计算边的注意力分数\n",
    "        graph.apply_edges(dgl.function.u_dot_v('ft', 'ft', 'a'))\n",
    "        # 对注意力分数进行归一化\n",
    "        graph.edata['sa'] = dgl.nn.functional.edge_softmax(\n",
    "            graph, graph.edata['a'] / self._out_feats ** 0.5)\n",
    "\n",
    "        # 聚合邻居节点的特征\n",
    "        graph.update_all(dgl.function.u_mul_e('ft_v', 'sa', 'attn'),\n",
    "                         dgl.function.sum('attn', 'agg_u'))\n",
    "\n",
    "        # 重塑聚合后的特征\n",
    "        rst = graph.dstdata['agg_u'].reshape(-1,\n",
    "                                             self._out_feats * self._num_heads)\n",
    "\n",
    "        # 如果使用跳跃连接\n",
    "        if self.skip_feat is not None:\n",
    "            skip_feat = self.skip_feat(feat[:graph.number_of_dst_nodes()])\n",
    "            # 如果使用门控机制\n",
    "            if self.gate is not None:\n",
    "                # 计算门控值\n",
    "                gate = torch.sigmoid(\n",
    "                    self.gate(\n",
    "                        torch.concat([skip_feat, rst, skip_feat - rst], dim=-1)))\n",
    "                # 结合跳跃连接和聚合后的特征\n",
    "                rst = gate * skip_feat + (1 - gate) * rst\n",
    "            else:\n",
    "                rst = skip_feat + rst\n",
    "\n",
    "        # 如果使用层归一化\n",
    "        if self.layer_norm is not None:\n",
    "            rst = self.layer_norm(rst)\n",
    "\n",
    "        # 如果使用激活函数\n",
    "        if self.activation is not None:\n",
    "            rst = self.activation(rst)\n",
    "\n",
    "        # 如果需要返回注意力分数\n",
    "        if get_attention:\n",
    "            return rst, graph.edata['sa']\n",
    "        else:\n",
    "            return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 GTAN 模型\n",
    "model = GTAN(in_feats, hidden_dim, n_layers, n_classes, heads)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7389988899230957, Train Acc: 0.5074999928474426, Test Acc: 0.4950000047683716\n",
      "Epoch 10, Loss: 0.6924529075622559, Train Acc: 0.5237500071525574, Test Acc: 0.5299999713897705\n",
      "Epoch 20, Loss: 0.6907057762145996, Train Acc: 0.5249999761581421, Test Acc: 0.5649999976158142\n",
      "Epoch 30, Loss: 0.6886590719223022, Train Acc: 0.543749988079071, Test Acc: 0.5299999713897705\n",
      "Epoch 40, Loss: 0.6877171993255615, Train Acc: 0.5325000286102295, Test Acc: 0.5299999713897705\n",
      "Epoch 50, Loss: 0.6867654323577881, Train Acc: 0.5450000166893005, Test Acc: 0.5249999761581421\n",
      "Epoch 60, Loss: 0.6858637928962708, Train Acc: 0.5425000190734863, Test Acc: 0.5249999761581421\n",
      "Epoch 70, Loss: 0.6848348379135132, Train Acc: 0.5537499785423279, Test Acc: 0.5099999904632568\n",
      "Epoch 80, Loss: 0.6834627389907837, Train Acc: 0.5550000071525574, Test Acc: 0.5149999856948853\n",
      "Epoch 90, Loss: 0.6815904378890991, Train Acc: 0.5612499713897705, Test Acc: 0.5099999904632568\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "train_model(model, g, features, labels, train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "源节点特征维度: 10, 目标节点特征维度: 10\n",
      "源节点特征维度: 10, 目标节点特征维度: 20\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "# 输入为整数的情况\n",
    "in_feats = 10\n",
    "src_feats, dst_feats = dgl.utils.expand_as_pair(in_feats)\n",
    "print(f\"源节点特征维度: {src_feats}, 目标节点特征维度: {dst_feats}\")\n",
    "\n",
    "# 输入为元组的情况\n",
    "in_feats = (10, 20)\n",
    "src_feats, dst_feats = dgl.utils.expand_as_pair(in_feats)\n",
    "print(f\"源节点特征维度: {src_feats}, 目标节点特征维度: {dst_feats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(network, self).__init__()\n",
    "    def __init__(self):\n",
    "        print(\"PosEncoding\")\n",
    "    def forward(self, pos):\n",
    "        print(\"PosEncoding forward\")\n",
    "        return 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PosEncoding\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'network' object has no attribute '_modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[1;32m--> 702\u001b[1;33m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\site-packages\\IPython\\lib\\pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    392\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                                 \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'__repr__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                             \u001b[1;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\site-packages\\IPython\\lib\\pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[1;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 700\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    701\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1932\u001b[0m             \u001b[0mextra_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1933\u001b[0m         \u001b[0mchild_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1934\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1935\u001b[0m             \u001b[0mmod_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1936\u001b[0m             \u001b[0mmod_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_addindent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1206\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m-> 1208\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m   1209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'network' object has no attribute '_modules'"
     ]
    }
   ],
   "source": [
    "network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(network, self).__init__()\n",
    "        print(\"init\")\n",
    "\n",
    "    def forward(self, pos):\n",
    "        print(\"forward\")\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "forward\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 实例化类\n",
    "net = network()\n",
    "# 调用forward方法，传入参数\n",
    "result = net(10)  # 这里传入10作为参数，实际传入的参数可以根据需要调整\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3, 4])\n",
      "Output tensor:\n",
      " tensor([[[  0.0615,   1.0786,  -1.8683,   0.7127],\n",
      "         [  4.1552,   0.0904,   1.1850,  -0.7272],\n",
      "         [ -2.1136,   0.8901,  -5.4305,   3.6585]],\n",
      "\n",
      "        [[  4.6945,  -1.2204,  -4.8430,  -2.2240],\n",
      "         [ 12.3335,   0.6931,   3.1643,  -5.3755],\n",
      "         [ -3.6933,   0.9422, -11.3417,   8.7332]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, cat_features):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.forward_mlp = nn.ModuleList(\n",
    "            [nn.Linear(4, 4) for i in range(len(cat_features))]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x 的形状假设为 (batch_size, num_features, feature_dim)\n",
    "        # 其中 num_features 是 cat_features 的长度，feature_dim 是每个特征的维度（这里是3）\n",
    "        outputs = []\n",
    "        for i, layer in enumerate(self.forward_mlp):\n",
    "            outputs.append(layer(x[:, i, :]))  # 对每个特征向量应用对应的全连接层\n",
    "        return torch.stack(outputs, dim=1)  # 将输出堆叠起来，形状为 (batch_size, num_features, feature_dim)\n",
    "\n",
    "# 示例\n",
    "cat_features = [1, 2, 3]  # 假设有 3 个类别特征\n",
    "model = MyModel(cat_features)\n",
    "\n",
    "# 创建一个输入张量，假设 batch_size=2，num_features=3，feature_dim=3\n",
    "input_tensor = torch.tensor([\n",
    "    [[1.0, 2.0, 3.0,4.0], \n",
    "     [4.0, 5.0, 6.0, 3.0], \n",
    "     [7.0, 8.0, 9.0, 3.0]],\n",
    "     \n",
    "    [[10.0, 11.0, 12.0, 3.0],\n",
    "     [13.0, 14.0, 15.0, 3.0],\n",
    "     [16.0, 17.0, 18.0, 3.0]]\n",
    "])\n",
    "\n",
    "# 前向传播\n",
    "output = model(input_tensor)\n",
    "print(\"Output shape:\", output.shape)  # 输出形状应该是 (2, 3, 3)\n",
    "print(\"Output tensor:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 126)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dgl.utils import expand_as_pair\n",
    "expand_as_pair(126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 126\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "in_src_feats, in_dst_feats = expand_as_pair(126)\n",
    "print(in_src_feats, in_dst_feats)\n",
    "print(type(in_src_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_src_feats =1\n",
    "in_src_feats, in_dst_feats = expand_as_pair(in_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4176\\3330451196.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;31m# Visualize the graph with attention scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m     \u001b[0mvisualize_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;31m# Visualize the attention scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4176\\3330451196.py\u001b[0m in \u001b[0;36mvisualize_graph\u001b[1;34m(graph, attention_scores)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;31m# Add edges with attention scores as labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf'{score:.2f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.nn import edge_softmax\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class TransformerConv(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 allow_zero_in_degree=False,\n",
    "                 skip_feat=True,\n",
    "                 gated=True,\n",
    "                 layer_norm=True,\n",
    "                 activation=nn.PReLU()):\n",
    "        \"\"\"\n",
    "        Initialize the transformer layer.\n",
    "        Attentional weights are jointly optimized in an end-to-end mechanism with graph neural networks and fraud detection networks.\n",
    "            :param in_feat: the shape of input feature\n",
    "            :param out_feats: the shape of output feature\n",
    "            :param num_heads: the number of multi-head attention \n",
    "            :param bias: whether to use bias\n",
    "            :param allow_zero_in_degree: whether to allow zero in degree\n",
    "            :param skip_feat: whether to skip some feature \n",
    "            :param gated: whether to use gate\n",
    "            :param layer_norm: whether to use layer regularization\n",
    "            :param activation: the type of activation function   \n",
    "        \"\"\"\n",
    "\n",
    "        super(TransformerConv, self).__init__()\n",
    "        self._in_src_feats, self._in_dst_feats = in_feats, in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._allow_zero_in_degree = allow_zero_in_degree\n",
    "        self._num_heads = num_heads\n",
    "\n",
    "        self.lin_query = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_key = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_value = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "\n",
    "        if skip_feat:\n",
    "            self.skip_feat = nn.Linear(\n",
    "                self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        else:\n",
    "            self.skip_feat = None\n",
    "        if gated:\n",
    "            self.gate = nn.Linear(\n",
    "                3 * self._out_feats * self._num_heads, 1, bias=bias)\n",
    "        else:\n",
    "            self.gate = None\n",
    "        if layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(self._out_feats * self._num_heads)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, graph, feat, get_attention=False):\n",
    "        \"\"\"\n",
    "        Description: Transformer Graph Convolution\n",
    "        :param graph: input graph\n",
    "            :param feat: input feat\n",
    "            :param get_attention: whether to get attention\n",
    "        \"\"\"\n",
    "\n",
    "        graph = graph.local_var()\n",
    "\n",
    "        if not self._allow_zero_in_degree:\n",
    "            if (graph.in_degrees() == 0).any():\n",
    "                raise ValueError('There are 0-in-degree nodes in the graph, '\n",
    "                                 'output for those nodes will be invalid. '\n",
    "                                 'This is harmful for some applications, '\n",
    "                                 'causing silent performance regression. '\n",
    "                                 'Adding self-loop on the input graph by '\n",
    "                                 'calling `g = dgl.add_self_loop(g)` will resolve '\n",
    "                                 'the issue. Setting ``allow_zero_in_degree`` '\n",
    "                                 'to be `True` when constructing this module will '\n",
    "                                 'suppress the check and let the code run.')\n",
    "\n",
    "        # check if feat is a tuple\n",
    "        if isinstance(feat, tuple):\n",
    "            h_src = feat[0]\n",
    "            h_dst = feat[1]\n",
    "        else:\n",
    "            h_src = feat\n",
    "            h_dst = h_src[:graph.number_of_dst_nodes()]\n",
    "\n",
    "        # Step 0. q, k, v\n",
    "        q_src = self.lin_query(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        k_dst = self.lin_key(h_dst).view(-1, self._num_heads, self._out_feats)\n",
    "        v_src = self.lin_value(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        # Assign features to nodes\n",
    "        graph.srcdata.update({'ft': q_src, 'ft_v': v_src})\n",
    "        graph.dstdata.update({'ft': k_dst})\n",
    "        # Step 1. dot product\n",
    "        graph.apply_edges(fn.u_dot_v('ft', 'ft', 'a'))\n",
    "\n",
    "        # Step 2. edge softmax to compute attention scores\n",
    "        graph.edata['sa'] = edge_softmax(\n",
    "            graph, graph.edata['a'] / self._out_feats ** 0.5)\n",
    "\n",
    "        # Step 3. Broadcast softmax value to each edge, and aggregate dst node\n",
    "        graph.update_all(fn.u_mul_e('ft_v', 'sa', 'attn'),\n",
    "                         fn.sum('attn', 'agg_u'))\n",
    "\n",
    "        # output results to the destination nodes\n",
    "        rst = graph.dstdata['agg_u'].reshape(-1,\n",
    "                                             self._out_feats * self._num_heads)\n",
    "\n",
    "        if self.skip_feat is not None:\n",
    "            skip_feat = self.skip_feat(feat[:graph.number_of_dst_nodes()])\n",
    "            if self.gate is not None:\n",
    "                gate = torch.sigmoid(\n",
    "                    self.gate(\n",
    "                        torch.concat([skip_feat, rst, skip_feat - rst], dim=-1)))\n",
    "                rst = gate * skip_feat + (1 - gate) * rst\n",
    "            else:\n",
    "                rst = skip_feat + rst\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            rst = self.layer_norm(rst)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            rst = self.activation(rst)\n",
    "\n",
    "        if get_attention:\n",
    "            return rst, graph.edata['sa']\n",
    "        else:\n",
    "            return rst\n",
    "\n",
    "\n",
    "def visualize_graph(graph, attention_scores=None):\n",
    "    g = graphviz.Digraph('G', filename='graph.gv')\n",
    "\n",
    "    # Add nodes\n",
    "    for i in range(graph.number_of_nodes()):\n",
    "        g.node(str(i))\n",
    "\n",
    "    # Add edges with attention scores as labels\n",
    "    if attention_scores is not None:\n",
    "        for i, (u, v) in enumerate(graph.edges()):\n",
    "            score = attention_scores[i].item()\n",
    "            g.edge(str(u.item()), str(v.item()), label=f'{score:.2f}')\n",
    "    else:\n",
    "        for u, v in graph.edges():\n",
    "            g.edge(str(u.item()), str(v.item()))\n",
    "\n",
    "    g.view()\n",
    "\n",
    "\n",
    "def visualize_attention_scores(attention_scores):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(attention_scores)), attention_scores.squeeze().tolist())\n",
    "    plt.xlabel('Edge Index')\n",
    "    plt.ylabel('Attention Score')\n",
    "    plt.title('Attention Scores')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a simple graph\n",
    "    src = torch.tensor([0, 1, 2])\n",
    "    dst = torch.tensor([1, 2, 0])\n",
    "    graph = dgl.graph((src, dst))\n",
    "\n",
    "    # Create node features\n",
    "    in_feats = 16\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    feat = torch.randn(num_nodes, in_feats)\n",
    "\n",
    "    # Create the TransformerConv layer\n",
    "    out_feats = 8\n",
    "    num_heads = 2\n",
    "    conv = TransformerConv(in_feats, out_feats, num_heads)\n",
    "\n",
    "    # Forward pass and get attention scores\n",
    "    output, attention_scores = conv(graph, feat, get_attention=True)\n",
    "\n",
    "    # Visualize the graph with attention scores\n",
    "    visualize_graph(graph, attention_scores)\n",
    "\n",
    "    # Visualize the attention scores\n",
    "    visualize_attention_scores(attention_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Could not open \"graph.gv.pdf\" for writing : Permission denied\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['dot', '-Kdot', '-Tpdf', '-O', 'graph.gv']' returned non-zero exit status 1. [stderr: b'Error: Could not open \"graph.gv.pdf\" for writing : Permission denied\\r\\n']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\site-packages\\graphviz\\backend\\execute.py\u001b[0m in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    443\u001b[0m             raise CalledProcessError(self.returncode, self.args, self.stdout,\n\u001b[1;32m--> 444\u001b[1;33m                                      self.stderr)\n\u001b[0m\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['dot', '-Kdot', '-Tpdf', '-O', 'graph.gv']' returned non-zero exit status 1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4176\\900014097.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;31m# Visualize the graph with attention scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     \u001b[0mvisualize_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;31m# Visualize the attention scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4176\\900014097.py\u001b[0m in \u001b[0;36mvisualize_graph\u001b[1;34m(graph, attention_scores)\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\site-packages\\graphviz\\_tools.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m                               category=category)\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\site-packages\\graphviz\\rendering.py\u001b[0m in \u001b[0;36mview\u001b[1;34m(self, filename, directory, cleanup, quiet, quiet_view)\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m    185\u001b[0m         return self.render(filename=filename, directory=directory, view=True,\n\u001b[1;32m--> 186\u001b[1;33m                            cleanup=cleanup, quiet=quiet, quiet_view=quiet_view)\n\u001b[0m",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\site-packages\\graphviz\\_tools.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m                               category=category)\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\site-packages\\graphviz\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mrendered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\site-packages\\graphviz\\_tools.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m                               category=category)\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\site-packages\\graphviz\\backend\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[0;32m    325\u001b[0m                       \u001b[0mcwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparts\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m                       \u001b[0mquiet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m                       capture_output=True)\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\baks\\lib\\site-packages\\graphviz\\backend\\execute.py\u001b[0m in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['dot', '-Kdot', '-Tpdf', '-O', 'graph.gv']' returned non-zero exit status 1. [stderr: b'Error: Could not open \"graph.gv.pdf\" for writing : Permission denied\\r\\n']"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.nn import edge_softmax\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class TransformerConv(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 allow_zero_in_degree=False,\n",
    "                 skip_feat=True,\n",
    "                 gated=True,\n",
    "                 layer_norm=True,\n",
    "                 activation=nn.PReLU()):\n",
    "        \"\"\"\n",
    "        Initialize the transformer layer.\n",
    "        Attentional weights are jointly optimized in an end-to-end mechanism with graph neural networks and fraud detection networks.\n",
    "            :param in_feat: the shape of input feature\n",
    "            :param out_feats: the shape of output feature\n",
    "            :param num_heads: the number of multi-head attention \n",
    "            :param bias: whether to use bias\n",
    "            :param allow_zero_in_degree: whether to allow zero in degree\n",
    "            :param skip_feat: whether to skip some feature \n",
    "            :param gated: whether to use gate\n",
    "            :param layer_norm: whether to use layer regularization\n",
    "            :param activation: the type of activation function   \n",
    "        \"\"\"\n",
    "\n",
    "        super(TransformerConv, self).__init__()\n",
    "        self._in_src_feats, self._in_dst_feats = in_feats, in_feats\n",
    "        self._out_feats = out_feats\n",
    "        self._allow_zero_in_degree = allow_zero_in_degree\n",
    "        self._num_heads = num_heads\n",
    "\n",
    "        self.lin_query = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_key = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_value = nn.Linear(\n",
    "            self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "\n",
    "        if skip_feat:\n",
    "            self.skip_feat = nn.Linear(\n",
    "                self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        else:\n",
    "            self.skip_feat = None\n",
    "        if gated:\n",
    "            self.gate = nn.Linear(\n",
    "                3 * self._out_feats * self._num_heads, 1, bias=bias)\n",
    "        else:\n",
    "            self.gate = None\n",
    "        if layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(self._out_feats * self._num_heads)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, graph, feat, get_attention=False):\n",
    "        \"\"\"\n",
    "        Description: Transformer Graph Convolution\n",
    "        :param graph: input graph\n",
    "            :param feat: input feat\n",
    "            :param get_attention: whether to get attention\n",
    "        \"\"\"\n",
    "\n",
    "        graph = graph.local_var()\n",
    "\n",
    "        if not self._allow_zero_in_degree:\n",
    "            if (graph.in_degrees() == 0).any():\n",
    "                raise ValueError('There are 0-in-degree nodes in the graph, '\n",
    "                                 'output for those nodes will be invalid. '\n",
    "                                 'This is harmful for some applications, '\n",
    "                                 'causing silent performance regression. '\n",
    "                                 'Adding self-loop on the input graph by '\n",
    "                                 'calling `g = dgl.add_self_loop(g)` will resolve '\n",
    "                                 'the issue. Setting ``allow_zero_in_degree`` '\n",
    "                                 'to be `True` when constructing this module will '\n",
    "                                 'suppress the check and let the code run.')\n",
    "\n",
    "        # check if feat is a tuple\n",
    "        if isinstance(feat, tuple):\n",
    "            h_src = feat[0]\n",
    "            h_dst = feat[1]\n",
    "        else:\n",
    "            h_src = feat\n",
    "            h_dst = h_src[:graph.number_of_dst_nodes()]\n",
    "\n",
    "        # Step 0. q, k, v\n",
    "        q_src = self.lin_query(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        k_dst = self.lin_key(h_dst).view(-1, self._num_heads, self._out_feats)\n",
    "        v_src = self.lin_value(\n",
    "            h_src).view(-1, self._num_heads, self._out_feats)\n",
    "        # Assign features to nodes\n",
    "        graph.srcdata.update({'ft': q_src, 'ft_v': v_src})\n",
    "        graph.dstdata.update({'ft': k_dst})\n",
    "        # Step 1. dot product\n",
    "        graph.apply_edges(fn.u_dot_v('ft', 'ft', 'a'))\n",
    "\n",
    "        # Step 2. edge softmax to compute attention scores\n",
    "        graph.edata['sa'] = edge_softmax(\n",
    "            graph, graph.edata['a'] / self._out_feats ** 0.5)\n",
    "\n",
    "        # Step 3. Broadcast softmax value to each edge, and aggregate dst node\n",
    "        graph.update_all(fn.u_mul_e('ft_v', 'sa', 'attn'),\n",
    "                         fn.sum('attn', 'agg_u'))\n",
    "\n",
    "        # output results to the destination nodes\n",
    "        rst = graph.dstdata['agg_u'].reshape(-1,\n",
    "                                             self._out_feats * self._num_heads)\n",
    "\n",
    "        if self.skip_feat is not None:\n",
    "            skip_feat = self.skip_feat(feat[:graph.number_of_dst_nodes()])\n",
    "            if self.gate is not None:\n",
    "                gate = torch.sigmoid(\n",
    "                    self.gate(\n",
    "                        torch.concat([skip_feat, rst, skip_feat - rst], dim=-1)))\n",
    "                rst = gate * skip_feat + (1 - gate) * rst\n",
    "            else:\n",
    "                rst = skip_feat + rst\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            rst = self.layer_norm(rst)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            rst = self.activation(rst)\n",
    "\n",
    "        if get_attention:\n",
    "            return rst, graph.edata['sa']\n",
    "        else:\n",
    "            return rst\n",
    "\n",
    "\n",
    "def visualize_graph(graph, attention_scores=None):\n",
    "    g = graphviz.Digraph('G', filename='graph.gv')\n",
    "\n",
    "    # Add nodes\n",
    "    for i in range(graph.number_of_nodes()):\n",
    "        g.node(str(i))\n",
    "\n",
    "    # Add edges with attention scores as labels\n",
    "    if attention_scores is not None:\n",
    "        # 对多头注意力分数进行平均\n",
    "        if len(attention_scores.shape) > 1:\n",
    "            attention_scores = attention_scores.mean(dim=1)\n",
    "        src, dst = graph.edges()\n",
    "        for i in range(len(src)):\n",
    "            score = attention_scores[i].item()\n",
    "            g.edge(str(src[i].item()), str(dst[i].item()), label=f'{score:.2f}')\n",
    "    else:\n",
    "        src, dst = graph.edges()\n",
    "        for i in range(len(src)):\n",
    "            g.edge(str(src[i].item()), str(dst[i].item()))\n",
    "\n",
    "    g.view()\n",
    "\n",
    "\n",
    "def visualize_attention_scores(attention_scores):\n",
    "    # 对多头注意力分数进行平均\n",
    "    if len(attention_scores.shape) > 1:\n",
    "        attention_scores = attention_scores.mean(dim=1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(attention_scores)), attention_scores.squeeze().tolist())\n",
    "    plt.xlabel('Edge Index')\n",
    "    plt.ylabel('Attention Score')\n",
    "    plt.title('Attention Scores')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a simple graph\n",
    "    src = torch.tensor([0, 1, 2])\n",
    "    dst = torch.tensor([1, 2, 0])\n",
    "    graph = dgl.graph((src, dst))\n",
    "\n",
    "    # Create node features\n",
    "    in_feats = 16\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    feat = torch.randn(num_nodes, in_feats)\n",
    "\n",
    "    # Create the TransformerConv layer\n",
    "    out_feats = 8\n",
    "    num_heads = 2\n",
    "    conv = TransformerConv(in_feats, out_feats, num_heads)\n",
    "\n",
    "    # Forward pass and get attention scores\n",
    "    output, attention_scores = conv(graph, feat, get_attention=True)\n",
    "\n",
    "    # Visualize the graph with attention scores\n",
    "    visualize_graph(graph, attention_scores)\n",
    "\n",
    "    # Visualize the attention scores\n",
    "    visualize_attention_scores(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
